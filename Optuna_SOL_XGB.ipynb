{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading ---\n",
      "Loading data from: SOL_minagg.csv...\n",
      "Loaded 22582 rows for SOL.\n",
      "\n",
      "--- Feature Engineering for SOL ---\n",
      "  Feature Eng Start: Initial rows = 22582\n",
      "  Initial NaNs check (essential cols):\n",
      "Series([], dtype: int64)\n",
      "  Rows after initial essential NaN drop: 22582\n",
      "  Calculating TA features (RSI, Stoch, MACD)...\n",
      "  Feature Eng End: Total columns = 41, Rows = 22582\n",
      "Feature engineering complete. Took 0.06s.\n",
      "\n",
      "--- Target Definition ---\n",
      "Defining target: 30m return >= -0.1%...\n",
      "  Raw positive target occurrence (before NaN drop): 57.66%\n",
      "\n",
      "--- Data Preparation ---\n",
      "\n",
      "--- NaN Counts Before Final Drop ---\n",
      "close_div_ma24h          359\n",
      "ma_1440m                 359\n",
      "lag_240m_price_return    241\n",
      "atr_840m                 209\n",
      "garman_klass_720m        179\n",
      "ma_720m                  179\n",
      "lag_120m_price_return    121\n",
      "ma_360m                   89\n",
      "lag_60m_price_return      61\n",
      "lag_60m_volume_return     61\n",
      "rolling_std_180m          45\n",
      "ma_180m                   44\n",
      "parkinson_180m            44\n",
      "MACDh_12_26_9             33\n",
      "lag_30m_volume_return     31\n",
      "lag_30m_price_return      31\n",
      "target_return_30m         30\n",
      "MACD_12_26_9              25\n",
      "lag_15m_volume_return     16\n",
      "lag_15m_price_return      16\n",
      "STOCHk_14_3_3             15\n",
      "RSI_14                    14\n",
      "lag_5m_volume_return       6\n",
      "lag_5m_price_return        6\n",
      "dtype: int64\n",
      "Total rows before final drop: 22582\n",
      "\n",
      "Applying final NaN/Inf Handling...\n",
      "NaN Handling: Dropped 389 rows with remaining NaNs.\n",
      "\n",
      "Final feature matrix shape: (22193, 32), Target shape: (22193,)\n",
      "Using 32 features.\n",
      "\n",
      "--- Starting SLIDING Window Backtest for SOL ---\n",
      "!!! Using Optuna (TimeSeriesSplit CV) + TA Features !!!\n",
      "Train Window: 360m, Step: 180m, Test Window: 72m, Optuna Trials: 50, CV Splits: 3\n",
      "\n",
      "--- Step 1 (2025-04-03 12:59:00) ---\n",
      "  Train: [0:359]; Test: [360:431]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 29.71s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 7, 'reg_alpha': 0.0032115838516769356, 'reg_lambda': 2.2582086896764437, 'gamma': 0.4159066809450932, 'subsample': 0.7696563325441095, 'colsample_bytree': 0.7280125404227962, 'learning_rate': 0.10744053575411942, 'scale_pos_weight': 1.7071373270273413}, Best CV F1(@0.5): 0.4029\n",
      "  Step 1 finished in 29.98s total.\n",
      "\n",
      "--- Step 2 (2025-04-03 15:59:00) ---\n",
      "  Train: [180:539]; Test: [540:611]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 31.88s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.043321637193547355, 'reg_lambda': 2.3726197044110684, 'gamma': 0.29107337380277437, 'subsample': 0.8605651353329864, 'colsample_bytree': 0.8610226672397611, 'learning_rate': 0.03584616005730301, 'scale_pos_weight': 4.284549081993224}, Best CV F1(@0.5): 0.3105\n",
      "  Step 2 finished in 32.18s total.\n",
      "\n",
      "--- Step 3 (2025-04-03 18:59:00) ---\n",
      "  Train: [360:719]; Test: [720:791]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 24.69s.\n",
      "  Best Params: {'max_depth': 8, 'min_child_weight': 1, 'reg_alpha': 0.3057097360752131, 'reg_lambda': 1.5371021032474272, 'gamma': 0.4935737207999852, 'subsample': 0.796021864769385, 'colsample_bytree': 0.8951686401144321, 'learning_rate': 0.042758153667102144, 'scale_pos_weight': 2.5401626139928624}, Best CV F1(@0.5): 0.5197\n",
      "  Step 3 finished in 25.00s total.\n",
      "\n",
      "--- Step 4 (2025-04-03 21:59:00) ---\n",
      "  Train: [540:899]; Test: [900:971]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 32.62s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 1, 'reg_alpha': 0.1458023826897053, 'reg_lambda': 3.135567524225071, 'gamma': 0.4332195541596576, 'subsample': 0.9110614308015378, 'colsample_bytree': 0.7927788115150655, 'learning_rate': 0.0930550662373465, 'scale_pos_weight': 4.979774411697031}, Best CV F1(@0.5): 0.8883\n",
      "  Step 4 finished in 32.86s total.\n",
      "\n",
      "--- Step 5 (2025-04-04 00:59:00) ---\n",
      "  Train: [720:1079]; Test: [1080:1151]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 25.54s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 2, 'reg_alpha': 0.009037424579078223, 'reg_lambda': 1.150694446723047, 'gamma': 0.39523979669184184, 'subsample': 0.9469406274761969, 'colsample_bytree': 0.7138628609273505, 'learning_rate': 0.09607926050645157, 'scale_pos_weight': 2.1206448610289783}, Best CV F1(@0.5): 0.7226\n",
      "  Step 5 finished in 25.82s total.\n",
      "\n",
      "--- Step 6 (2025-04-04 03:59:00) ---\n",
      "  Train: [900:1259]; Test: [1260:1331]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 26.94s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 2, 'reg_alpha': 0.018212106015904755, 'reg_lambda': 2.3560143591965925, 'gamma': 0.19731422095695378, 'subsample': 0.9835913601062048, 'colsample_bytree': 0.876754155618999, 'learning_rate': 0.11011811994022706, 'scale_pos_weight': 3.575601740873156}, Best CV F1(@0.5): 0.6219\n",
      "  Step 6 finished in 27.19s total.\n",
      "\n",
      "--- Step 7 (2025-04-04 06:59:00) ---\n",
      "  Train: [1080:1439]; Test: [1440:1511]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 28.28s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 7, 'reg_alpha': 0.01946382502722615, 'reg_lambda': 8.301045911449407, 'gamma': 0.11408531781784403, 'subsample': 0.866615788422083, 'colsample_bytree': 0.8598264230691883, 'learning_rate': 0.03848394351030951, 'scale_pos_weight': 2.9322433198818114}, Best CV F1(@0.5): 0.8069\n",
      "  Step 7 finished in 28.53s total.\n",
      "\n",
      "--- Step 8 (2025-04-04 09:59:00) ---\n",
      "  Train: [1260:1619]; Test: [1620:1691]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 24.68s.\n",
      "  Best Params: {'max_depth': 8, 'min_child_weight': 7, 'reg_alpha': 0.08507125630853003, 'reg_lambda': 1.289972682730311, 'gamma': 0.4713415524527565, 'subsample': 0.7542988496222892, 'colsample_bytree': 0.8704125680639585, 'learning_rate': 0.08151724594993925, 'scale_pos_weight': 4.387742268958694}, Best CV F1(@0.5): 0.7368\n",
      "  Step 8 finished in 24.92s total.\n",
      "\n",
      "--- Step 9 (2025-04-04 12:59:00) ---\n",
      "  Train: [1440:1799]; Test: [1800:1871]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 25.96s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 1, 'reg_alpha': 0.13106748038256444, 'reg_lambda': 9.346019965672179, 'gamma': 0.21922311465743738, 'subsample': 0.7995849277258316, 'colsample_bytree': 0.674351973024497, 'learning_rate': 0.030653881660850847, 'scale_pos_weight': 4.996328793960301}, Best CV F1(@0.5): 0.7914\n",
      "  Step 9 finished in 26.28s total.\n",
      "\n",
      "--- Step 10 (2025-04-04 15:59:00) ---\n",
      "  Train: [1620:1979]; Test: [1980:2051]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 31.13s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 1, 'reg_alpha': 0.0022546265478664822, 'reg_lambda': 4.50568199743181, 'gamma': 0.3774925347075253, 'subsample': 0.8544634122824936, 'colsample_bytree': 0.7598295965141614, 'learning_rate': 0.03802896757907691, 'scale_pos_weight': 4.409271637061976}, Best CV F1(@0.5): 0.8443\n",
      "  Step 10 finished in 31.42s total.\n",
      "\n",
      "--- Step 11 (2025-04-04 18:59:00) ---\n",
      "  Train: [1800:2159]; Test: [2160:2231]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 30.74s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 7, 'reg_alpha': 0.07355888190046783, 'reg_lambda': 6.536661459481877, 'gamma': 0.09766071017163777, 'subsample': 0.9189930910944539, 'colsample_bytree': 0.7716932367346597, 'learning_rate': 0.11514391362006411, 'scale_pos_weight': 4.434356537896646}, Best CV F1(@0.5): 0.7476\n",
      "  Step 11 finished in 31.04s total.\n",
      "\n",
      "--- Step 12 (2025-04-04 21:59:00) ---\n",
      "  Train: [1980:2339]; Test: [2340:2411]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 32.25s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.25337738214562594, 'reg_lambda': 7.818660771841467, 'gamma': 0.10854473851420482, 'subsample': 0.8313570820857197, 'colsample_bytree': 0.878389981423806, 'learning_rate': 0.11644849900537724, 'scale_pos_weight': 4.36365233370918}, Best CV F1(@0.5): 0.8254\n",
      "  Step 12 finished in 32.57s total.\n",
      "\n",
      "--- Step 13 (2025-04-05 00:59:00) ---\n",
      "  Train: [2160:2519]; Test: [2520:2591]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 32.09s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 1, 'reg_alpha': 0.001028080957194193, 'reg_lambda': 2.2225621057785334, 'gamma': 0.1527980791732671, 'subsample': 0.8059404507521467, 'colsample_bytree': 0.7813277202051805, 'learning_rate': 0.06006017878949289, 'scale_pos_weight': 4.96001088717205}, Best CV F1(@0.5): 0.6750\n",
      "  Step 13 finished in 32.38s total.\n",
      "\n",
      "--- Step 14 (2025-04-05 03:59:00) ---\n",
      "  Train: [2340:2699]; Test: [2700:2771]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 32.25s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.023953936395005777, 'reg_lambda': 4.59396239690898, 'gamma': 0.3373205877256776, 'subsample': 0.9816413774709368, 'colsample_bytree': 0.6406543149752568, 'learning_rate': 0.1021646062539785, 'scale_pos_weight': 3.993874374081985}, Best CV F1(@0.5): 0.4375\n",
      "  Step 14 finished in 32.52s total.\n",
      "\n",
      "--- Step 15 (2025-04-05 06:59:00) ---\n",
      "  Train: [2520:2879]; Test: [2880:2951]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 33.27s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 0.016476948838513288, 'reg_lambda': 3.11402551067318, 'gamma': 0.07237965729191892, 'subsample': 0.888756131549518, 'colsample_bytree': 0.6733109377695033, 'learning_rate': 0.030089473023003313, 'scale_pos_weight': 2.419049289896248}, Best CV F1(@0.5): 0.7275\n",
      "  Step 15 finished in 33.59s total.\n",
      "\n",
      "--- Step 16 (2025-04-05 09:59:00) ---\n",
      "  Train: [2700:3059]; Test: [3060:3131]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 30.65s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 2, 'reg_alpha': 0.0010242938077854144, 'reg_lambda': 4.225332542701432, 'gamma': 0.45099233068513506, 'subsample': 0.944854153434316, 'colsample_bytree': 0.777795148862993, 'learning_rate': 0.09313536065778955, 'scale_pos_weight': 4.6517191821951265}, Best CV F1(@0.5): 0.7821\n",
      "  Step 16 finished in 30.90s total.\n",
      "\n",
      "--- Step 17 (2025-04-05 12:59:00) ---\n",
      "  Train: [2880:3239]; Test: [3240:3311]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 26.04s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 1, 'reg_alpha': 0.31766689602259235, 'reg_lambda': 6.959626977261699, 'gamma': 0.3580034302168503, 'subsample': 0.7795664634033257, 'colsample_bytree': 0.6167150197105205, 'learning_rate': 0.08313799641867341, 'scale_pos_weight': 1.727618262879283}, Best CV F1(@0.5): 0.5988\n",
      "  Step 17 finished in 26.29s total.\n",
      "\n",
      "--- Step 18 (2025-04-05 15:59:00) ---\n",
      "  Train: [3060:3419]; Test: [3420:3491]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 27.73s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 6, 'reg_alpha': 0.003546593835696567, 'reg_lambda': 7.2514317653104925, 'gamma': 0.35169963693478035, 'subsample': 0.8993034620757313, 'colsample_bytree': 0.8525399556729575, 'learning_rate': 0.035136202941891764, 'scale_pos_weight': 3.7769751346007006}, Best CV F1(@0.5): 0.3758\n",
      "  Step 18 finished in 28.03s total.\n",
      "\n",
      "--- Step 19 (2025-04-05 18:59:00) ---\n",
      "  Train: [3240:3599]; Test: [3600:3671]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 28.69s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 5, 'reg_alpha': 0.029801045593685557, 'reg_lambda': 9.925676497028368, 'gamma': 0.4801221094208574, 'subsample': 0.7917237803295352, 'colsample_bytree': 0.6015579038687711, 'learning_rate': 0.078147417652878, 'scale_pos_weight': 4.176414334281336}, Best CV F1(@0.5): 0.7012\n",
      "  Step 19 finished in 28.90s total.\n",
      "\n",
      "--- Step 20 (2025-04-05 21:59:00) ---\n",
      "  Train: [3420:3779]; Test: [3780:3851]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 27.57s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 4, 'reg_alpha': 0.012696481917922324, 'reg_lambda': 4.095798387922755, 'gamma': 0.2955171625125315, 'subsample': 0.8104670401838086, 'colsample_bytree': 0.7082456557629525, 'learning_rate': 0.03154308900905849, 'scale_pos_weight': 4.491019224764313}, Best CV F1(@0.5): 0.8751\n",
      "  Step 20 finished in 27.96s total.\n",
      "\n",
      "--- Step 21 (2025-04-06 00:59:00) ---\n",
      "  Train: [3600:3959]; Test: [3960:4031]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 29.61s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 3, 'reg_alpha': 0.0023252762302358064, 'reg_lambda': 6.679372418645101, 'gamma': 0.20767588806219278, 'subsample': 0.7220590713633444, 'colsample_bytree': 0.7461814666399758, 'learning_rate': 0.03593157376339938, 'scale_pos_weight': 4.886619039960242}, Best CV F1(@0.5): 0.9381\n",
      "  Step 21 finished in 29.83s total.\n",
      "\n",
      "--- Step 22 (2025-04-06 03:59:00) ---\n",
      "  Train: [3780:4139]; Test: [4140:4211]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 21.83s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 4, 'reg_alpha': 0.0040850238463027285, 'reg_lambda': 4.730544722805812, 'gamma': 0.30846743592254644, 'subsample': 0.8173533937179377, 'colsample_bytree': 0.7098213644216415, 'learning_rate': 0.08044855219967981, 'scale_pos_weight': 1.9840587089541528}, Best CV F1(@0.5): 0.7931\n",
      "  Step 22 finished in 22.14s total.\n",
      "\n",
      "--- Step 23 (2025-04-06 06:59:00) ---\n",
      "  Train: [3960:4319]; Test: [4320:4391]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 29.15s.\n",
      "  Best Params: {'max_depth': 8, 'min_child_weight': 4, 'reg_alpha': 0.17857488349088352, 'reg_lambda': 8.268800711253482, 'gamma': 0.4237257760702402, 'subsample': 0.748401305894187, 'colsample_bytree': 0.6419638858878357, 'learning_rate': 0.03520243927324802, 'scale_pos_weight': 4.560017488175139}, Best CV F1(@0.5): 0.7456\n",
      "  Step 23 finished in 29.45s total.\n",
      "\n",
      "--- Step 24 (2025-04-06 09:59:00) ---\n",
      "  Train: [4140:4499]; Test: [4500:4571]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 22.27s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 7, 'reg_alpha': 0.17358468625640627, 'reg_lambda': 2.198727057600806, 'gamma': 0.1690398867285523, 'subsample': 0.9882249564071048, 'colsample_bytree': 0.6002880050654222, 'learning_rate': 0.11187430673993114, 'scale_pos_weight': 4.954522967123962}, Best CV F1(@0.5): 0.7128\n",
      "  Step 24 finished in 22.49s total.\n",
      "\n",
      "--- Step 25 (2025-04-06 12:59:00) ---\n",
      "  Train: [4320:4679]; Test: [4680:4751]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 26.68s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 7, 'reg_alpha': 0.34525232043142856, 'reg_lambda': 3.3428108998841815, 'gamma': 0.4485780837131121, 'subsample': 0.7048979758621599, 'colsample_bytree': 0.7678027013490928, 'learning_rate': 0.12151153356291483, 'scale_pos_weight': 2.591686547999681}, Best CV F1(@0.5): 0.5694\n",
      "  Step 25 finished in 26.87s total.\n",
      "\n",
      "--- Step 26 (2025-04-06 15:59:00) ---\n",
      "  Train: [4500:4859]; Test: [4860:4931]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 27.29s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 3, 'reg_alpha': 0.3412868344975946, 'reg_lambda': 5.399682282514697, 'gamma': 0.28795233035492773, 'subsample': 0.7526393339099704, 'colsample_bytree': 0.700153263778769, 'learning_rate': 0.05695618722161011, 'scale_pos_weight': 2.19926901988673}, Best CV F1(@0.5): 0.3936\n",
      "  Step 26 finished in 27.52s total.\n",
      "\n",
      "--- Step 27 (2025-04-06 18:59:00) ---\n",
      "  Train: [4680:5039]; Test: [5040:5111]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 30.48s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 3, 'reg_alpha': 0.019683954386435783, 'reg_lambda': 1.203992400080619, 'gamma': 0.22550476425616425, 'subsample': 0.8751263263038046, 'colsample_bytree': 0.7300104677730758, 'learning_rate': 0.030035910387878513, 'scale_pos_weight': 1.411419646414411}, Best CV F1(@0.5): 0.3769\n",
      "  Step 27 finished in 30.82s total.\n",
      "\n",
      "--- Step 28 (2025-04-06 21:59:00) ---\n",
      "  Train: [4860:5219]; Test: [5220:5291]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 31.05s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 2, 'reg_alpha': 0.0010066423394183394, 'reg_lambda': 4.72787188879792, 'gamma': 0.15967378863226572, 'subsample': 0.9874854523684405, 'colsample_bytree': 0.876325147276946, 'learning_rate': 0.03634612232856025, 'scale_pos_weight': 4.155305618005673}, Best CV F1(@0.5): 0.3564\n",
      "  Step 28 finished in 31.45s total.\n",
      "\n",
      "--- Step 29 (2025-04-07 00:59:00) ---\n",
      "  Train: [5040:5399]; Test: [5400:5471]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 35.65s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 1, 'reg_alpha': 0.021861580293294443, 'reg_lambda': 1.0794286221388665, 'gamma': 0.3694733475492364, 'subsample': 0.953475555258529, 'colsample_bytree': 0.8201499698542191, 'learning_rate': 0.050861147114232945, 'scale_pos_weight': 3.925027637905238}, Best CV F1(@0.5): 0.6715\n",
      "  Step 29 finished in 36.04s total.\n",
      "\n",
      "--- Step 30 (2025-04-07 03:59:00) ---\n",
      "  Train: [5220:5579]; Test: [5580:5651]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 24.70s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 7, 'reg_alpha': 0.048977467481763945, 'reg_lambda': 3.0135817356104164, 'gamma': 0.17422572491362986, 'subsample': 0.7707649180167986, 'colsample_bytree': 0.6433848564316433, 'learning_rate': 0.07487843802580715, 'scale_pos_weight': 4.71439979052611}, Best CV F1(@0.5): 0.6903\n",
      "  Step 30 finished in 24.88s total.\n",
      "\n",
      "--- Step 31 (2025-04-07 06:59:00) ---\n",
      "  Train: [5400:5759]; Test: [5760:5831]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 29.13s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 4, 'reg_alpha': 0.0188682545315322, 'reg_lambda': 8.021411683543162, 'gamma': 0.18160753448408753, 'subsample': 0.7464308415841377, 'colsample_bytree': 0.7217718305734556, 'learning_rate': 0.05821766304432423, 'scale_pos_weight': 1.984206530147681}, Best CV F1(@0.5): 0.5456\n",
      "  Step 31 finished in 29.38s total.\n",
      "\n",
      "--- Step 32 (2025-04-07 09:59:00) ---\n",
      "  Train: [5580:5939]; Test: [5940:6011]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 33.62s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 2, 'reg_alpha': 0.001880603942907059, 'reg_lambda': 1.582273308179183, 'gamma': 0.10136182088959157, 'subsample': 0.8227504241881187, 'colsample_bytree': 0.646062012872454, 'learning_rate': 0.03701399780878441, 'scale_pos_weight': 4.4280854773273575}, Best CV F1(@0.5): 0.6197\n",
      "  Step 32 finished in 33.92s total.\n",
      "\n",
      "--- Step 33 (2025-04-07 12:59:00) ---\n",
      "  Train: [5760:6119]; Test: [6120:6191]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 32.51s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 1, 'reg_alpha': 0.009843090789546758, 'reg_lambda': 6.777575525773481, 'gamma': 0.4434663343243763, 'subsample': 0.9390429070729045, 'colsample_bytree': 0.7648803146466885, 'learning_rate': 0.09783579417145692, 'scale_pos_weight': 2.164168747127695}, Best CV F1(@0.5): 0.4979\n",
      "  Step 33 finished in 32.75s total.\n",
      "\n",
      "--- Step 34 (2025-04-07 15:59:00) ---\n",
      "  Train: [5940:6299]; Test: [6300:6371]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 32.72s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 1, 'reg_alpha': 0.03649562196434126, 'reg_lambda': 6.073601961212675, 'gamma': 0.3343223598309688, 'subsample': 0.9513272264963462, 'colsample_bytree': 0.8719960509676867, 'learning_rate': 0.041827546157790434, 'scale_pos_weight': 1.4028663698654755}, Best CV F1(@0.5): 0.4235\n",
      "  Step 34 finished in 33.02s total.\n",
      "\n",
      "--- Step 35 (2025-04-07 18:59:00) ---\n",
      "  Train: [6120:6479]; Test: [6480:6551]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 29.02s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 3, 'reg_alpha': 0.23397024877388423, 'reg_lambda': 6.24919483616556, 'gamma': 0.28617923432425646, 'subsample': 0.8737974813294918, 'colsample_bytree': 0.7347201245317109, 'learning_rate': 0.038343912587044275, 'scale_pos_weight': 2.084974637005943}, Best CV F1(@0.5): 0.6116\n",
      "  Step 35 finished in 29.33s total.\n",
      "\n",
      "--- Step 36 (2025-04-07 21:59:00) ---\n",
      "  Train: [6300:6659]; Test: [6660:6731]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 30.54s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 3, 'reg_alpha': 0.1335512059152547, 'reg_lambda': 9.861584183973088, 'gamma': 0.47032960865463985, 'subsample': 0.985775344609308, 'colsample_bytree': 0.7987045421367529, 'learning_rate': 0.03318410214830488, 'scale_pos_weight': 4.734607179048574}, Best CV F1(@0.5): 0.4607\n",
      "  Step 36 finished in 30.83s total.\n",
      "\n",
      "--- Step 37 (2025-04-08 00:59:00) ---\n",
      "  Train: [6480:6839]; Test: [6840:6911]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 24.70s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 3, 'reg_alpha': 0.001277577799686875, 'reg_lambda': 1.3207650200779022, 'gamma': 0.4750939538875414, 'subsample': 0.7050950217709637, 'colsample_bytree': 0.7877550802967829, 'learning_rate': 0.03148458405866547, 'scale_pos_weight': 0.5870994760144077}, Best CV F1(@0.5): 0.4154\n",
      "  Step 37 finished in 24.92s total.\n",
      "\n",
      "--- Step 38 (2025-04-08 03:59:00) ---\n",
      "  Train: [6660:7019]; Test: [7020:7091]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n",
      "  Optuna finished in 27.07s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 1, 'reg_alpha': 0.005480332826470312, 'reg_lambda': 4.78733985157666, 'gamma': 0.48579092874038476, 'subsample': 0.9045651963542423, 'colsample_bytree': 0.8707741320911854, 'learning_rate': 0.031091617688053626, 'scale_pos_weight': 1.3860917312260406}, Best CV F1(@0.5): 0.5685\n",
      "  Step 38 finished in 27.39s total.\n",
      "\n",
      "--- Step 39 (2025-04-08 06:59:00) ---\n",
      "  Train: [6840:7199]; Test: [7200:7271]\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit, scoring F1@0.5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-18 15:48:43,508] Trial 46 failed with parameters: {'max_depth': 9, 'min_child_weight': 3, 'reg_alpha': 0.030494571003714478, 'reg_lambda': 2.633715754574309, 'gamma': 0.19958191759985042, 'subsample': 0.8072583968814686, 'colsample_bytree': 0.7781758716355935, 'learning_rate': 0.03526870305067796, 'scale_pos_weight': 1.6029533048512334} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Local\\Temp\\ipykernel_2496\\48757139.py\", line 349, in <lambda>\n",
      "    obj_func = lambda trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, cv_strategy)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Local\\Temp\\ipykernel_2496\\48757139.py\", line 209, in objective\n",
      "    model.fit(X_train_fold, y_train_fold, verbose=False)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py\", line 1490, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-18 15:48:43,509] Trial 46 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 351\u001b[0m\n\u001b[0;32m    348\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m    349\u001b[0m obj_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, cv_strategy)\n\u001b[1;32m--> 351\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_OPTUNA_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m best_params_step \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m    354\u001b[0m best_score_step \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[5], line 349\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    347\u001b[0m sampler \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;66;03m# Seed sampler based on step index\u001b[39;00m\n\u001b[0;32m    348\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[1;32m--> 349\u001b[0m obj_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_roll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_roll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXGB_FIXED_PARAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(obj_func, n_trials\u001b[38;5;241m=\u001b[39mN_OPTUNA_TRIALS, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    353\u001b[0m best_params_step \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "Cell \u001b[1;32mIn[5], line 209\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X, y, fixed_params, cv_strategy)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train_fold)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m: cv_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0.0\u001b[39m); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    208\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[1;32m--> 209\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m preds_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_fold)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    211\u001b[0m preds_binary \u001b[38;5;241m=\u001b[39m (preds_proba \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m OPTUNA_EVAL_THRESHOLD)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# B3_Configurable_Minute_SOL_Optuna_V4_TA_DebugNaN.py # <-- Renamed\n",
    "# Adds TA features, adjusts target, includes NaN diagnostics.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas_ta as ta\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "CSV_FILE_PATH = 'SOL_minagg.csv'; SYMBOL_NAME = 'SOL'\n",
    "PREDICTION_WINDOW_MINUTES = int(0.5 * 60); TARGET_THRESHOLD_PCT = -0.1\n",
    "TRAIN_WINDOW_MINUTES = 6 * 60; STEP_MINUTES = 3 * 60; TEST_WINDOW_FRACTION = 0.2\n",
    "XGB_FIXED_PARAMS = {\"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"use_label_encoder\": False, \"random_state\":42, \"tree_method\":\"gpu_hist\", \"predictor\":\"gpu_predictor\", \"gpu_id\":0, \"n_jobs\":-1, \"n_estimators\":150}\n",
    "N_OPTUNA_TRIALS = 50; OPTUNA_CV_SPLITS = 3; OPTUNA_EVAL_THRESHOLD = 0.50\n",
    "PROBABILITY_THRESHOLD_RANGE = (0.05, 0.95); PROBABILITY_THRESHOLD_STEP = 0.05\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Derived Variables ---\n",
    "# ==============================================================================\n",
    "TEST_WINDOW_MINUTES = max(1, int(TEST_WINDOW_FRACTION * TRAIN_WINDOW_MINUTES))\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(PROBABILITY_THRESHOLD_RANGE[0], PROBABILITY_THRESHOLD_RANGE[1], PROBABILITY_THRESHOLD_STEP)\n",
    "epsilon = 1e-9 # Define epsilon globally for reuse\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Feature Engineering Functions (with initial NaN check) ---\n",
    "# ==============================================================================\n",
    "def garman_klass_volatility_min(o, h, l, c, window_min):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl=np.log(h/l.replace(0, np.nan)); log_co=np.log(c/o.replace(0, np.nan))\n",
    "    gk = 0.5*(log_hl**2) - (2*np.log(2)-1)*(log_co**2); gk = gk.fillna(0)\n",
    "    min_p = max(1, window_min // 4); rm = gk.rolling(window_min, min_periods=min_p).mean(); rm = rm.clip(lower=0); return np.sqrt(rm)\n",
    "def parkinson_volatility_min(h, l, window_min):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl_sq = np.log(h/l.replace(0, np.nan))**2\n",
    "    log_hl_sq = log_hl_sq.fillna(0); min_p = max(1, window_min // 4); rs = log_hl_sq.rolling(window_min, min_periods=min_p).sum()\n",
    "    f = 1/(4*np.log(2)*window_min) if window_min>0 else 0; return np.sqrt(f*rs)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Feature Engineering Functions (with TA + NaN Fix + Diagnostics) ---\n",
    "# ==============================================================================\n",
    "# Define epsilon globally if not already done, or pass it\n",
    "epsilon = 1e-9\n",
    "\n",
    "def garman_klass_volatility_min(o, h, l, c, window_min):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl=np.log(h/l.replace(0, np.nan)); log_co=np.log(c/o.replace(0, np.nan))\n",
    "    gk = 0.5*(log_hl**2) - (2*np.log(2)-1)*(log_co**2); gk = gk.fillna(0)\n",
    "    min_p = max(1, window_min // 4); rm = gk.rolling(window_min, min_periods=min_p).mean(); rm = rm.clip(lower=0); return np.sqrt(rm)\n",
    "\n",
    "def parkinson_volatility_min(h, l, window_min):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl_sq = np.log(h/l.replace(0, np.nan))**2\n",
    "    log_hl_sq = log_hl_sq.fillna(0); min_p = max(1, window_min // 4); rs = log_hl_sq.rolling(window_min, min_periods=min_p).sum()\n",
    "    f = 1/(4*np.log(2)*window_min) if window_min>0 else 0; return np.sqrt(f*rs)\n",
    "\n",
    "def calculate_features_min_with_ta(df_input):\n",
    "    \"\"\"\n",
    "    Calculates features including TA indicators (RSI, Stoch, MACD).\n",
    "    Removed ma4h_div_ma24h due to missing prerequisite MA.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    print(f\"  Feature Eng Start: Initial rows = {len(df)}\")\n",
    "\n",
    "    # --- Robust Initial NaN Check ---\n",
    "    essential_cols = ['open', 'high', 'low', 'close', 'volumefrom']\n",
    "    initial_nan_check = df[essential_cols].isnull().sum()\n",
    "    print(f\"  Initial NaNs check (essential cols):\\n{initial_nan_check[initial_nan_check > 0]}\")\n",
    "    df = df.dropna(subset=essential_cols)\n",
    "    print(f\"  Rows after initial essential NaN drop: {len(df)}\")\n",
    "    if df.empty: print(\"  Error: Empty DF after essential NaN drop.\"); return df\n",
    "    # --- End Initial Check ---\n",
    "\n",
    "    # Ensure base columns are numeric\n",
    "    base_cols_numeric = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']\n",
    "    for col in base_cols_numeric:\n",
    "        if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else: print(f\"  Warning: Missing base column '{col}'\"); df[col] = 0\n",
    "    # Re-check NaNs just in case coerce created some\n",
    "    df = df.dropna(subset=essential_cols)\n",
    "    if df.empty: print(\"  Error: Empty DF after numeric conversion NaN drop.\"); return df\n",
    "\n",
    "    # --- Feature Calculations ---\n",
    "    df['price_change_1m_temp'] = df['close'].pct_change(periods=1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) * 100\n",
    "        df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "    df['garman_klass_720m'] = garman_klass_volatility_min(df['open'], df['high'], df['low'], df['close'], 12 * 60)\n",
    "    df['parkinson_180m'] = parkinson_volatility_min(df['high'], df['low'], 3 * 60)\n",
    "    min_periods_rolling = 2\n",
    "    df['ma_180m'] = df['close'].rolling(3 * 60, min_periods=max(min_periods_rolling, (3*60)//4)).mean()\n",
    "    df['rolling_std_180m'] = df['price_change_1m_temp'].rolling(3 * 60, min_periods=max(min_periods_rolling, (3*60)//4)).std() * 100\n",
    "    lag_periods_price_min = [5, 15, 30, 60, 120, 240]; lag_periods_volume_min = [5, 15, 30, 60]\n",
    "    for lag in lag_periods_price_min: df[f'lag_{lag}m_price_return'] = df['price_change_1m_temp'].shift(lag) * 100\n",
    "    df['volume_return_1m'] = df['volumefrom'].pct_change(periods=1).replace([np.inf, -np.inf], 0) * 100\n",
    "    for lag in lag_periods_volume_min: df[f'lag_{lag}m_volume_return'] = df['volume_return_1m'].shift(lag)\n",
    "\n",
    "    # MAs (Reduced longest to 24h = 1440m)\n",
    "    ma_periods_min = [6*60, 12*60, 24*60]; min_p_long = 50\n",
    "    for p in ma_periods_min: df[f'ma_{p}m'] = df['close'].rolling(p, min_periods=max(min_p_long, p//4)).mean()\n",
    "\n",
    "    # Ratios using available MAs\n",
    "    # --- REMOVED ma4h_div_ma24h calculation BLOCK ---\n",
    "\n",
    "    # Keep close / 24h MA ratio\n",
    "    if 'close' in df and 'ma_1440m' in df: # Check 'ma_1440m' exists now\n",
    "        df['close_div_ma24h'] = df['close']/(df['ma_1440m'].replace(0, epsilon)+epsilon)\n",
    "    else: df['close_div_ma24h']=np.nan\n",
    "\n",
    "    # ATR (Reduced periods)\n",
    "    df['prev_close']=df['close'].shift(1); df['hml']=df['high']-df['low']; df['hmpc']=np.abs(df['high']-df['prev_close']); df['lmpc']=np.abs(df['low']-df['prev_close'])\n",
    "    df['tr']=df[['hml','hmpc','lmpc']].max(axis=1); atr_periods_min = [14*60]; min_p_atr = 20\n",
    "    for p in atr_periods_min: df[f'atr_{p}m'] = df['tr'].rolling(p, min_periods=max(min_p_atr, p//4)).mean()\n",
    "    df = df.drop(columns=['prev_close', 'hml', 'hmpc', 'lmpc', 'tr'], errors='ignore')\n",
    "\n",
    "    # --- ADDING TA FEATURES ---\n",
    "    print(\"  Calculating TA features (RSI, Stoch, MACD)...\")\n",
    "    try:\n",
    "        min_ta_warmup = 30\n",
    "        if len(df) < min_ta_warmup:\n",
    "            print(f\"  Warning: Insufficient data ({len(df)} rows) for TA warmup ({min_ta_warmup}). Skipping TA features.\")\n",
    "        else:\n",
    "            # Calculate TA features only if enough data exists\n",
    "            df.ta.rsi(length=14, append=True)\n",
    "            df.ta.stoch(k=14, d=3, smooth_k=3, append=True)\n",
    "            df.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "            # Create derived TA features only if base TA cols were created\n",
    "            if 'RSI_14' in df.columns:\n",
    "                df['rsi_14_oversold'] = (df['RSI_14'] < 30).astype(int)\n",
    "                df['rsi_14_overbought'] = (df['RSI_14'] > 70).astype(int)\n",
    "            if 'STOCHk_14_3_3' in df.columns:\n",
    "                df['stoch_k_oversold'] = (df['STOCHk_14_3_3'] < 20).astype(int)\n",
    "                df['stoch_k_overbought'] = (df['STOCHk_14_3_3'] > 80).astype(int)\n",
    "            if 'MACDh_12_26_9' in df.columns:\n",
    "                 df['macd_hist_positive'] = (df['MACDh_12_26_9'] > 0).astype(int)\n",
    "\n",
    "            # Simple RSI Divergence Proxy\n",
    "            if 'RSI_14' in df.columns: # Check RSI exists before calculating divergence\n",
    "                for n_div in [30, 60]:\n",
    "                    if len(df) > n_div:\n",
    "                        min_price_n = df['low'].rolling(window=n_div, min_periods=n_div//2).min()\n",
    "                        min_rsi_n = df['RSI_14'].rolling(window=n_div, min_periods=n_div//2).min()\n",
    "                        price_lower_low = df['low'] < min_price_n.shift(1)\n",
    "                        rsi_higher_low = df['RSI_14'] > min_rsi_n.shift(1)\n",
    "                        df[f'rsi_bull_div_{n_div}m'] = (price_lower_low & rsi_higher_low).astype(int)\n",
    "                    else:\n",
    "                         df[f'rsi_bull_div_{n_div}m'] = 0\n",
    "\n",
    "    except Exception as e_ta:\n",
    "        print(f\"!! Error calculating TA features: {e_ta}\")\n",
    "        # This might still leave NaNs if calculation failed mid-way\n",
    "\n",
    "    # --- Cleanup Intermediate ---\n",
    "    cols_to_drop_intermediate = ['price_change_1m_temp', 'volume_return_1m']\n",
    "    df = df.drop(columns=[col for col in cols_to_drop_intermediate if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Final check for infinities\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    if df[numeric_cols].isin([np.inf, -np.inf]).any().any():\n",
    "        print(\"  Warning: Infinities detected after feature calculation. Replacing with NaN.\")\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    print(f\"  Feature Eng End: Total columns = {df.shape[1]}, Rows = {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Optuna Objective Function (Unchanged)---\n",
    "# ==============================================================================\n",
    "def objective(trial, X, y, fixed_params, cv_strategy):\n",
    "    param = {\n",
    "        \"max_depth\":        trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
    "        \"reg_alpha\":        trial.suggest_float(\"reg_alpha\", 1e-3, 0.5, log=True),\n",
    "        \"reg_lambda\":       trial.suggest_float(\"reg_lambda\", 1.0, 10.0, log=True),\n",
    "        \"gamma\":            trial.suggest_float(\"gamma\", 0, 0.5),\n",
    "        \"subsample\":        trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.03, 0.15, log=True),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 5.0),\n",
    "    }\n",
    "    xgb_params = {**fixed_params, **param}\n",
    "    cv_scores = []\n",
    "    try:\n",
    "        y_np = y.to_numpy() if isinstance(y, pd.Series) else np.array(y)\n",
    "        X_np = X.to_numpy() if isinstance(X, pd.DataFrame) else np.array(X)\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X_np, y_np)): # Use the passed cv_strategy\n",
    "            X_train_fold, X_val_fold = X_np[train_idx], X_np[val_idx]\n",
    "            y_train_fold, y_val_fold = y_np[train_idx], y_np[val_idx]\n",
    "            if len(np.unique(y_train_fold)) < 2: cv_scores.append(0.0); continue\n",
    "            model = xgb.XGBClassifier(**xgb_params)\n",
    "            model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            preds_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "            preds_binary = (preds_proba >= OPTUNA_EVAL_THRESHOLD).astype(int)\n",
    "            f1 = f1_score(y_val_fold, preds_binary, zero_division=0)\n",
    "            cv_scores.append(f1)\n",
    "            trial.report(f1, fold)\n",
    "            if trial.should_prune(): raise optuna.exceptions.TrialPruned()\n",
    "        average_f1 = np.mean(cv_scores) if cv_scores else 0.0\n",
    "    except optuna.exceptions.TrialPruned: raise\n",
    "    except Exception as e: print(f\"Error in Optuna trial {trial.number}, fold {fold}: {e}\"); return 0.0\n",
    "    return average_f1 if not np.isnan(average_f1) else 0.0\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Main Script Logic ---\n",
    "# ==============================================================================\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"--- Data Loading ---\"); print(f\"Loading data from: {CSV_FILE_PATH}...\")\n",
    "try:\n",
    "    df_data = pd.read_csv(CSV_FILE_PATH, parse_dates=['timestamp'])\n",
    "    df_data = df_data.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df_data)} rows for {SYMBOL_NAME}.\")\n",
    "except Exception as e: print(f\"Error loading data: {e}\"); exit()\n",
    "if len(df_data) < 3000: print(f\"Warning: Initial data size ({len(df_data)}) might be small for long features/windows.\"); # Heuristic check\n",
    "# Adjusted check for minimum data needed for at least one backtest step\n",
    "min_data_needed = TRAIN_WINDOW_MINUTES + TEST_WINDOW_MINUTES + STEP_MINUTES # Rough minimum for one step\n",
    "if len(df_data) < min_data_needed: print(f\"Error: Insufficient initial data ({len(df_data)} < {min_data_needed} needed).\"); exit()\n",
    "\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(f\"\\n--- Feature Engineering for {SYMBOL_NAME} ---\")\n",
    "start_fe = time.time()\n",
    "df_data = calculate_features_min_with_ta(df_data) # Call the updated function\n",
    "if df_data.empty: print(f\"Error: Feature calculation resulted in empty DataFrame.\"); exit()\n",
    "print(f\"Feature engineering complete. Took {time.time() - start_fe:.2f}s.\")\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"\\n--- Target Definition ---\")\n",
    "print(f\"Defining target: {PREDICTION_WINDOW_MINUTES}m return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "target_col = f'target_return_{PREDICTION_WINDOW_MINUTES}m'\n",
    "df_data[target_col] = df_data['close'].shift(-PREDICTION_WINDOW_MINUTES).sub(df_data['close']).div(df_data['close'].replace(0, np.nan)).mul(100)\n",
    "target_occurrence = df_data[target_col].notna() & (df_data[target_col] >= TARGET_THRESHOLD_PCT)\n",
    "print(f\"  Raw positive target occurrence (before NaN drop): {target_occurrence.mean()*100:.2f}%\")\n",
    "\n",
    "# --- 4. Prepare Data for Modeling ---\n",
    "print(\"\\n--- Data Preparation ---\")\n",
    "base_cols_ohlcv = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']\n",
    "cols_to_keep_final = ['timestamp', target_col]\n",
    "potential_feature_cols = [col for col in df_data.columns if col not in cols_to_keep_final and col not in base_cols_ohlcv and not col.startswith('STOCHd') and not col.startswith('MACDs')] # Exclude some redundant TA cols if needed\n",
    "numeric_feature_cols = df_data[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "final_feature_cols = numeric_feature_cols\n",
    "if not final_feature_cols: print(\"Error: No numeric features found after selection.\"); exit()\n",
    "\n",
    "cols_to_select = final_feature_cols + [col for col in cols_to_keep_final if col in df_data.columns]\n",
    "df_model_ready = df_data[cols_to_select].copy()\n",
    "\n",
    "# --- DIAGNOSE NANS ---\n",
    "print(\"\\n--- NaN Counts Before Final Drop ---\")\n",
    "nan_counts = df_model_ready[final_feature_cols + [target_col]].isnull().sum()\n",
    "print(nan_counts[nan_counts > 0].sort_values(ascending=False))\n",
    "print(f\"Total rows before final drop: {len(df_model_ready)}\")\n",
    "# --- END DIAGNOSIS ---\n",
    "\n",
    "print(\"\\nApplying final NaN/Inf Handling...\")\n",
    "initial_rows = len(df_model_ready)\n",
    "# Drop rows with NaNs in features OR target\n",
    "df_model_ready = df_model_ready.dropna(subset=final_feature_cols + [target_col])\n",
    "final_rows = len(df_model_ready)\n",
    "print(f\"NaN Handling: Dropped {initial_rows - final_rows} rows with remaining NaNs.\")\n",
    "\n",
    "# Final Inf check\n",
    "if df_model_ready[final_feature_cols].isin([np.inf, -np.inf]).any().any():\n",
    "    inf_count = df_model_ready[final_feature_cols].isin([np.inf, -np.inf]).sum().sum()\n",
    "    print(f\"Replacing {inf_count} final infinites...\")\n",
    "    df_model_ready.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    rows_b4 = len(df_model_ready); df_model_ready = df_model_ready.dropna(subset=final_feature_cols)\n",
    "    print(f\"Dropped {rows_b4 - len(df_model_ready)} more rows after Inf handling.\")\n",
    "\n",
    "if df_model_ready.empty: print(f\"Error: DataFrame empty after final NaN/Inf handling.\"); exit()\n",
    "\n",
    "X = df_model_ready[final_feature_cols]\n",
    "y_binary = (df_model_ready[target_col] >= TARGET_THRESHOLD_PCT).astype(int)\n",
    "timestamps = df_model_ready['timestamp']\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}, Target shape: {y_binary.shape}\")\n",
    "print(f\"Using {len(final_feature_cols)} features.\")\n",
    "# print(f\"Final Feature List: {final_feature_cols}\") # Uncomment to verify features\n",
    "\n",
    "# --- 5. SLIDING Window Backtesting ---\n",
    "# ... (Rest of the script - backtesting loop, evaluation, plotting - remains unchanged from V3_TSS) ...\n",
    "# ... it will use the new X and y_binary calculated above ...\n",
    "\n",
    "print(f\"\\n--- Starting SLIDING Window Backtest for {SYMBOL_NAME} ---\")\n",
    "print(f\"!!! Using Optuna (TimeSeriesSplit CV) + TA Features !!!\") # <--- Updated print\n",
    "\n",
    "if len(X) < TRAIN_WINDOW_MINUTES + STEP_MINUTES:\n",
    "     print(f\"Error: Not enough data after pre-processing ({len(X)} rows) for train window ({TRAIN_WINDOW_MINUTES}) + step ({STEP_MINUTES}).\");\n",
    "     exit()\n",
    "\n",
    "all_predictions_proba = []; all_actual = []; backtest_timestamps = []\n",
    "all_best_params = []\n",
    "num_steps = 0\n",
    "start_index_loop = TRAIN_WINDOW_MINUTES # Start after the first full training window\n",
    "end_index_loop = len(X) - TEST_WINDOW_MINUTES + 1 # Ensure space for the last test window\n",
    "\n",
    "print(f\"Train Window: {TRAIN_WINDOW_MINUTES}m, Step: {STEP_MINUTES}m, Test Window: {TEST_WINDOW_MINUTES}m, Optuna Trials: {N_OPTUNA_TRIALS}, CV Splits: {OPTUNA_CV_SPLITS}\")\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for i in range(start_index_loop, end_index_loop, STEP_MINUTES):\n",
    "    step_start_time = time.time()\n",
    "    train_idx_start = i - TRAIN_WINDOW_MINUTES\n",
    "    train_idx_end = i\n",
    "    test_idx_start = i\n",
    "    test_idx_end = min(i + TEST_WINDOW_MINUTES, len(X))\n",
    "\n",
    "    if test_idx_start >= test_idx_end: print(f\"Stopping loop: Test window invalid.\"); break\n",
    "\n",
    "    X_train_roll = X.iloc[train_idx_start : train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[train_idx_start : train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx_start : test_idx_end]\n",
    "    y_test_roll_actual_series = y_binary.iloc[test_idx_start : test_idx_end]\n",
    "    step_timestamps = timestamps.iloc[test_idx_start : test_idx_end]\n",
    "\n",
    "    if y_test_roll_actual_series.empty: print(f\"Warning: Step {i}, empty test actuals.\"); continue\n",
    "    current_timestamp = step_timestamps.iloc[0]\n",
    "    if X_train_roll.empty or len(np.unique(y_train_roll)) < 2: print(f\"Warning: Step {i}, invalid training data.\"); continue\n",
    "\n",
    "    print(f\"\\n--- Step {num_steps + 1} ({current_timestamp}) ---\")\n",
    "    print(f\"  Train: [{train_idx_start}:{train_idx_end-1}]; Test: [{test_idx_start}:{test_idx_end-1}]\")\n",
    "\n",
    "    # --- Hyperparameter Tuning with Optuna ---\n",
    "    print(f\"  Running Optuna ({N_OPTUNA_TRIALS} trials, cv={OPTUNA_CV_SPLITS} TimeSeriesSplit, scoring F1@{OPTUNA_EVAL_THRESHOLD})...\")\n",
    "    optuna_start_time = time.time()\n",
    "    try:\n",
    "        cv_strategy = TimeSeriesSplit(n_splits=OPTUNA_CV_SPLITS)\n",
    "        pruner = optuna.pruners.MedianPruner(n_warmup_steps=5, n_min_trials=10)\n",
    "        sampler = optuna.samplers.TPESampler(seed=i) # Seed sampler based on step index\n",
    "        study = optuna.create_study(direction='maximize', pruner=pruner, sampler=sampler)\n",
    "        obj_func = lambda trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, cv_strategy)\n",
    "\n",
    "        study.optimize(obj_func, n_trials=N_OPTUNA_TRIALS, n_jobs=1, show_progress_bar=False)\n",
    "\n",
    "        best_params_step = study.best_params\n",
    "        best_score_step = study.best_value\n",
    "\n",
    "        print(f\"  Optuna finished in {time.time() - optuna_start_time:.2f}s.\")\n",
    "        print(f\"  Best Params: {best_params_step}, Best CV F1(@{OPTUNA_EVAL_THRESHOLD}): {best_score_step:.4f}\")\n",
    "        all_best_params.append({'step': num_steps + 1, 'params': best_params_step, 'cv_f1': best_score_step, 'timestamp': current_timestamp})\n",
    "\n",
    "        # --- Fit final model for the step ---\n",
    "        final_model_params = {**XGB_FIXED_PARAMS, **best_params_step}\n",
    "        model_roll = xgb.XGBClassifier(**final_model_params)\n",
    "        model_roll.fit(X_train_roll, y_train_roll, verbose=False)\n",
    "\n",
    "        # --- Predict probabilities for the test window ---\n",
    "        prob_roll_window = model_roll.predict_proba(X_test_roll)[:, 1]\n",
    "\n",
    "        # --- Store results ---\n",
    "        all_predictions_proba.extend(prob_roll_window)\n",
    "        all_actual.extend(y_test_roll_actual_series.tolist())\n",
    "        backtest_timestamps.extend(step_timestamps.tolist())\n",
    "        num_steps += 1\n",
    "\n",
    "    except ValueError as ve: print(f\"!! Value Error at step {i}: {ve}\"); continue\n",
    "    except Exception as e_step: print(f\"!! Error at step {i}: {e_step}\"); traceback.print_exc(); continue\n",
    "\n",
    "    step_end_time = time.time()\n",
    "    print(f\"  Step {num_steps} finished in {step_end_time - step_start_time:.2f}s total.\")\n",
    "\n",
    "loop_end_time = time.time()\n",
    "print(f\"\\nBacktesting loop finished. Completed {num_steps} steps in {(loop_end_time - loop_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results with PTT ---\n",
    "if num_steps > 0 and len(all_predictions_proba) == len(all_actual) and len(all_predictions_proba) == len(backtest_timestamps):\n",
    "    print(f\"\\n--- Evaluating Results for {SYMBOL_NAME} with Probability Threshold Tuning ---\")\n",
    "    print(f\"Threshold search range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    best_threshold = 0.5; best_f1_thresh = -1.0\n",
    "    results_per_threshold = {}\n",
    "    probabilities_np = np.array(all_predictions_proba)\n",
    "    actual_np = np.array(all_actual)\n",
    "\n",
    "    for t in THRESHOLD_SEARCH_RANGE:\n",
    "        predictions_thresh = (probabilities_np >= t).astype(int)\n",
    "        if np.sum(actual_np) == 0 and np.sum(predictions_thresh) == 0: acc_t, pre_t, rec_t, f1_t = 1.0, 1.0, 1.0, 1.0\n",
    "        elif np.sum(actual_np) > 0 and np.sum(predictions_thresh) == 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        elif np.sum(actual_np) == 0 and np.sum(predictions_thresh) > 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh); pre_t = precision_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             rec_t = recall_score(actual_np, predictions_thresh, zero_division=0); f1_t = f1_score(actual_np, predictions_thresh, zero_division=0)\n",
    "        results_per_threshold[round(t, 2)] = {'f1': f1_t, 'acc': acc_t, 'pre': pre_t, 'rec': rec_t}\n",
    "        if f1_t >= best_f1_thresh:\n",
    "             if f1_t > best_f1_thresh or abs(t - 0.5) < abs(best_threshold - 0.5): best_f1_thresh = f1_t; best_threshold = t\n",
    "\n",
    "    print(f\"\\nBest Threshold for {SYMBOL_NAME} found: {best_threshold:.2f} (Yielding F1 Score: {best_f1_thresh:.4f})\")\n",
    "    final_predictions_optimized = (probabilities_np >= best_threshold).astype(int)\n",
    "    final_accuracy = accuracy_score(actual_np, final_predictions_optimized)\n",
    "    final_precision = precision_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_recall = recall_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_f1 = f1_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "\n",
    "    print(f\"\\n--- Final Performance Metrics for {SYMBOL_NAME} (Optimized Threshold) ---\")\n",
    "    print(f\"Target: {PREDICTION_WINDOW_MINUTES}m return >= {TARGET_THRESHOLD_PCT}%\")\n",
    "    print(f\"Windowing: Train={TRAIN_WINDOW_MINUTES}m, Step={STEP_MINUTES}m, Test Window={TEST_WINDOW_MINUTES}m\")\n",
    "    print(f\"Hyperparameter Tuning: Optuna ({N_OPTUNA_TRIALS} trials/step, TimeSeriesSplit CV, F1@{OPTUNA_EVAL_THRESHOLD})\") # Updated print\n",
    "    print(f\"Total Individual Predictions Evaluated: {len(actual_np)}\")\n",
    "    print(f\"Positive Target Occurrence (final eval set): {actual_np.mean()*100:.2f}%\") # Occurrence in the actual evaluated set\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "    if OPTUNA_EVAL_THRESHOLD in results_per_threshold:\n",
    "        res_eval = results_per_threshold[OPTUNA_EVAL_THRESHOLD]\n",
    "        print(f\"(Compare: Optuna Eval {OPTUNA_EVAL_THRESHOLD} Thresh -> F1:{res_eval['f1']:.4f}, Acc:{res_eval['acc']:.4f}, Pre:{res_eval['pre']:.4f}, Rec:{res_eval['rec']:.4f})\")\n",
    "    elif 0.5 in results_per_threshold:\n",
    "        res_def = results_per_threshold[0.5]\n",
    "        print(f\"(Compare: Default 0.5 Thresh -> F1:{res_def['f1']:.4f}, Acc:{res_def['acc']:.4f}, Pre:{res_def['pre']:.4f}, Rec:{res_def['rec']:.4f})\")\n",
    "\n",
    "    results_summary = { # Store results\n",
    "        'symbol': SYMBOL_NAME, 'probabilities': probabilities_np, 'actuals': actual_np, 'timestamps': backtest_timestamps,\n",
    "        'best_threshold': best_threshold, 'metrics_optimized': {'acc': final_accuracy, 'pre': final_precision, 'rec': final_recall, 'f1': final_f1},\n",
    "        'metrics_default_0.5': results_per_threshold.get(0.5, {}), 'best_params_per_step': all_best_params, 'results_per_threshold': results_per_threshold\n",
    "    }\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy ---\n",
    "    print(f\"\\nPlotting cumulative accuracy for {SYMBOL_NAME} (optimized threshold)...\")\n",
    "    try:\n",
    "        cumulative_accuracy_list_optimized = (np.cumsum(final_predictions_optimized == actual_np) / np.arange(1, len(actual_np) + 1))\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(backtest_timestamps, cumulative_accuracy_list_optimized, marker='.', linestyle='-', markersize=1, alpha=0.7, label=f'Cumulative Accuracy ({SYMBOL_NAME})')\n",
    "        rolling_window_plot_size = max(TEST_WINDOW_MINUTES * 5, 60 * 12)\n",
    "        if len(actual_np) > rolling_window_plot_size:\n",
    "             results_df = pd.DataFrame({'correct': (final_predictions_optimized == actual_np).astype(int)}, index=pd.to_datetime(backtest_timestamps))\n",
    "             try: rolling_acc = results_df['correct'].rolling(window=rolling_window_plot_size, min_periods=rolling_window_plot_size//2).mean()\n",
    "             except Exception as e_roll: print(f\"Could not calculate rolling accuracy: {e_roll}\"); rolling_acc = None\n",
    "             if rolling_acc is not None: plt.plot(rolling_acc.index, rolling_acc, linestyle='--', color='red', label=f'Rolling Acc ({rolling_window_plot_size} min window)')\n",
    "        plt.title(f'{SYMBOL_NAME} Backtest (Optuna TSS CV + TA Features, Train:{TRAIN_WINDOW_MINUTES}m) - Best Thresh: {best_threshold:.2f}') # Update title\n",
    "        plt.xlabel('Timestamp'); plt.ylabel('Accuracy'); min_y_plot=max(0.0, np.min(cumulative_accuracy_list_optimized)-0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.4)\n",
    "        max_y_plot=min(1.0, np.max(cumulative_accuracy_list_optimized)+0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.8)\n",
    "        if max_y_plot - min_y_plot < 0.1: mid_point=(max_y_plot+min_y_plot)/2; min_y_plot=max(0.0, mid_point-0.05); max_y_plot=min(1.0, mid_point+0.05)\n",
    "        plt.ylim(min_y_plot, max_y_plot); plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "        plot_filename = f\"backtest_accuracy_{SYMBOL_NAME}_2h_target_Optuna_TSS_TA.png\" # Update filename for new target/features\n",
    "        plt.savefig(plot_filename); print(f\"Saved accuracy plot to {plot_filename}\"); plt.close()\n",
    "    except Exception as e_plot: print(f\"Error plotting: {e_plot}\")\n",
    "\n",
    "elif num_steps == 0: print(f\"No backtesting steps completed.\")\n",
    "else: print(f\"Error: Length mismatch in results arrays. Cannot evaluate.\")\n",
    "\n",
    "# --- End of Script ---\n",
    "print(f\"\\n{'='*30} Overall Script Finished for {SYMBOL_NAME} {'='*30}\")\n",
    "overall_end_time = time.time()\n",
    "print(f\"Total execution time: {(overall_end_time - overall_start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
       "       'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
       "       'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
       "       'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
       "       'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
       "       'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
       "       'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
       "       'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
       "       'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
       "       'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
       "       'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
       "       'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
       "       'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
