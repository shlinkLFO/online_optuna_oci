{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading ---\n",
      "Loading data from: SOL_minagg.csv (expecting SOL data without 'symbol' column)\n",
      "Loaded 22582 rows for SOL.\n",
      "\n",
      "--- Feature Engineering for SOL ---\n",
      "Feature engineering for SOL complete. Took 0.02 seconds. Columns: 48\n",
      "\n",
      "--- Target Definition ---\n",
      "Defining target as 720m future return >= 2.5%...\n",
      "\n",
      "--- Data Preparation ---\n",
      "Applying NaN/Inf Handling...\n",
      "Dropping 1 columns with >30% NaN values: ['lag_10080m_price_return']\n",
      "NaN Handling: Dropped 1 columns. Dropped 5041 rows with remaining NaNs.\n",
      "\n",
      "Final feature matrix shape for SOL: (17541, 40)\n",
      "Target vector shape for SOL: (17541,)\n",
      "Using 40 features.\n",
      "\n",
      "--- Starting SLIDING Window Backtest for SOL with Optuna (50 trials/step) ---\n",
      "Train Window: 1440 mins, Step: 60 mins, Test Window: 288 mins\n",
      "Warning: Skipping step 1440 for SOL. Invalid training data.\n",
      "Warning: Skipping step 1500 for SOL. Invalid training data.\n",
      "\n",
      "--- SOL - Step 1 (Predicting window starting 2025-04-07 03:01:00) ---\n",
      "  Training indices: [120:1559]; Testing indices: [1560:1847]\n",
      "  Running Optuna (50 trials, 3-fold CV, maximizing F1@0.5)...\n",
      "  Optuna finished in 21.19s.\n",
      "  Best Params (suggested): {'learning_rate': 0.05871078765951933, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.712765862343972, 'colsample_bytree': 0.8619201998981703, 'scale_pos_weight': 3}\n",
      "  Best CV F1 Score (at 0.5 thresh): 0.0000\n",
      "  Step 1 finished in 21.36s total.\n",
      "\n",
      "--- SOL - Step 2 (Predicting window starting 2025-04-07 04:01:00) ---\n",
      "  Training indices: [180:1619]; Testing indices: [1620:1907]\n",
      "  Running Optuna (50 trials, 3-fold CV, maximizing F1@0.5)...\n",
      "  Optuna finished in 29.86s.\n",
      "  Best Params (suggested): {'learning_rate': 0.05344386934380271, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.9656893529724296, 'colsample_bytree': 0.7060126646398518, 'scale_pos_weight': 3}\n",
      "  Best CV F1 Score (at 0.5 thresh): 0.9406\n",
      "  Step 2 finished in 30.04s total.\n",
      "\n",
      "--- SOL - Step 3 (Predicting window starting 2025-04-07 05:01:00) ---\n",
      "  Training indices: [240:1679]; Testing indices: [1680:1967]\n",
      "  Running Optuna (50 trials, 3-fold CV, maximizing F1@0.5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-18 14:18:19,776] Trial 37 failed with parameters: {'learning_rate': 0.08106390436713795, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.8631254012133971, 'colsample_bytree': 0.7624090848033295, 'scale_pos_weight': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Local\\Temp\\ipykernel_29944\\2283494743.py\", line 337, in <lambda>\n",
      "    obj_func = lambda trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, N_CV_FOLDS)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Local\\Temp\\ipykernel_29944\\2283494743.py\", line 187, in objective\n",
      "    model.fit(X_tr, y_tr, verbose=False)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py\", line 1490, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-18 14:18:19,804] Trial 37 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 340\u001b[0m\n\u001b[0;32m    337\u001b[0m obj_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, N_CV_FOLDS)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[1;32m--> 340\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_OPTUNA_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Set timeout (seconds) if needed\u001b[39;00m\n\u001b[0;32m    342\u001b[0m best_params_step \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m    343\u001b[0m best_score_step \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value \u001b[38;5;66;03m# Best average F1@0.5 found in CV\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 337\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    331\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    332\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    333\u001b[0m     pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m# Prune after 5 trials\u001b[39;00m\n\u001b[0;32m    334\u001b[0m )\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Define the objective function with necessary arguments for this step\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_roll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_roll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXGB_FIXED_PARAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_CV_FOLDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[0;32m    340\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(obj_func, n_trials\u001b[38;5;241m=\u001b[39mN_OPTUNA_TRIALS, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m# Set timeout (seconds) if needed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 187\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X_train, y_train, fixed_params, n_folds)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 187\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     preds_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# --- Evaluate based on F1 score at a fixed threshold (e.g., 0.5) ---\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# This provides a consistent score for Optuna to optimize.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# The final model will still undergo PTT later.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# B3_Configurable_Minute_SOL_Optuna.py\n",
    "# Predictor with Sliding Window, Per-Step HParam Tuning (Optuna), and PTT\n",
    "# Adapted for minute-level data from SOL_minagg.csv (single symbol).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna # <--- ADDED: Optuna import\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# GridSearchCV no longer needed\n",
    "from sklearn.model_selection import StratifiedKFold # Keep StratifiedKFold for CV inside Optuna objective\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "# Reduce Optuna logging verbosity\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Data ---\n",
    "CSV_FILE_PATH = 'SOL_minagg.csv'\n",
    "SYMBOL_NAME = 'SOL'\n",
    "\n",
    "# --- Target Definition ---\n",
    "PREDICTION_WINDOW_MINUTES = 12 * 60\n",
    "TARGET_THRESHOLD_PCT = 2.5\n",
    "\n",
    "# --- Backtesting Windowing (Defined in MINUTES) ---\n",
    "TRAIN_WINDOW_MINUTES = 24 * 60\n",
    "STEP_MINUTES = 1 * 60\n",
    "TEST_WINDOW_FRACTION = 0.2\n",
    "\n",
    "# --- Model & Tuning ---\n",
    "\n",
    "# Fixed parameters (always active for XGBoost)\n",
    "XGB_FIXED_PARAMS = {\n",
    "    \"objective\":        \"binary:logistic\",\n",
    "    \"eval_metric\":      \"logloss\",\n",
    "    \"use_label_encoder\": False,\n",
    "    \"random_state\":      42,\n",
    "    \"tree_method\":      \"gpu_hist\",     # Use GPU\n",
    "    \"predictor\":        \"gpu_predictor\",# Use GPU\n",
    "    \"gpu_id\":            0,\n",
    "    \"n_jobs\":           -1,             # Allow XGBoost to use cores for host-side tasks\n",
    "    # Some parameters previously in grid search can be fixed here if desired\n",
    "    \"n_estimators\":     150,            # Fix n_estimators, tune learning_rate\n",
    "    \"reg_alpha\":        0.1,            # Example fixed L1\n",
    "    \"reg_lambda\":       4.0,            # Example fixed L2\n",
    "    \"gamma\":            0.1,            # Example fixed gamma\n",
    "}\n",
    "\n",
    "# --- Optuna Configuration ---\n",
    "N_OPTUNA_TRIALS = 50           # Number of trials Optuna runs per step\n",
    "N_CV_FOLDS = 3                  # Number of folds for cross-validation within Optuna objective\n",
    "\n",
    "# Probability Threshold Tuning Range\n",
    "PROBABILITY_THRESHOLD_RANGE = (0.10, 0.90)\n",
    "PROBABILITY_THRESHOLD_STEP = 0.05\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Derived Variables (Do not change these directly) ---\n",
    "# ==============================================================================\n",
    "TEST_WINDOW_MINUTES = max(1, int(TEST_WINDOW_FRACTION * TRAIN_WINDOW_MINUTES))\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(\n",
    "    PROBABILITY_THRESHOLD_RANGE[0],\n",
    "    PROBABILITY_THRESHOLD_RANGE[1],\n",
    "    PROBABILITY_THRESHOLD_STEP\n",
    ")\n",
    "# grid_combinations no longer directly relevant, Optuna uses N_OPTUNA_TRIALS\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Feature Engineering Functions (Minute Based - Unchanged) ---\n",
    "# ==============================================================================\n",
    "def garman_klass_volatility_min(o, h, l, c, window_min):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl=np.log(h/l.replace(0,np.nan)); log_co=np.log(c/o.replace(0,np.nan))\n",
    "    gk = 0.5*(log_hl**2) - (2*np.log(2)-1)*(log_co**2); gk = gk.fillna(0)\n",
    "    min_p = max(1, window_min // 4); rm = gk.rolling(window_min, min_periods=min_p).mean(); rm = rm.clip(lower=0); return np.sqrt(rm)\n",
    "def parkinson_volatility_min(h, l, window_min):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): log_hl_sq = np.log(h/l.replace(0,np.nan))**2\n",
    "    log_hl_sq = log_hl_sq.fillna(0); min_p = max(1, window_min // 4); rs = log_hl_sq.rolling(window_min, min_periods=min_p).sum()\n",
    "    f = 1/(4*np.log(2)*window_min) if window_min>0 else 0; return np.sqrt(f*rs)\n",
    "def calculate_features_min(df_input):\n",
    "    df = df_input.copy()\n",
    "    base_cols_numeric = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']\n",
    "    for col in base_cols_numeric:\n",
    "        if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else: print(f\"Warning: Missing base column '{col}'\"); df[col] = 0\n",
    "    if df[['open', 'high', 'low', 'close']].isnull().any().any():\n",
    "        df = df.dropna(subset=['open', 'high', 'low', 'close'])\n",
    "    if df.empty: return df\n",
    "    df['price_change_1m_temp'] = df['close'].pct_change(periods=1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df['price_range_pct'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan) * 100\n",
    "        df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "    df['garman_klass_720m'] = garman_klass_volatility_min(df['open'], df['high'], df['low'], df['close'], 12 * 60)\n",
    "    df['parkinson_180m'] = parkinson_volatility_min(df['high'], df['low'], 3 * 60)\n",
    "    min_periods_rolling = 2\n",
    "    df['ma_180m'] = df['close'].rolling(3 * 60, min_periods=max(min_periods_rolling, (3*60)//4)).mean()\n",
    "    df['rolling_std_180m'] = df['close'].rolling(3 * 60, min_periods=max(min_periods_rolling, (3*60)//4)).std()\n",
    "    lag_periods_price_min = [3*60, 6*60, 12*60, 24*60, 48*60, 72*60, 168*60]\n",
    "    lag_periods_volume_min = [3*60, 6*60, 12*60, 24*60]\n",
    "    for lag in lag_periods_price_min: df[f'lag_{lag}m_price_return'] = df['price_change_1m_temp'].shift(lag) * 100\n",
    "    df['volume_return_1m'] = df['volumefrom'].pct_change(periods=1) * 100\n",
    "    for lag in lag_periods_volume_min: df[f'lag_{lag}m_volume_return'] = df['volume_return_1m'].shift(lag)\n",
    "    ma_periods_min = [6*60, 12*60, 24*60, 48*60, 72*60, 168*60]\n",
    "    std_periods_min = [6*60, 12*60, 24*60, 48*60, 72*60, 168*60]\n",
    "    min_p_long = 50\n",
    "    for p in ma_periods_min: df[f'ma_{p}m'] = df['close'].rolling(p, min_periods=max(min_p_long, p//4)).mean()\n",
    "    for p in std_periods_min: df[f'rolling_std_{p}m'] = df['price_change_1m_temp'].rolling(p, min_periods=max(min_p_long, p//4)).std() * 100\n",
    "    df['prev_close']=df['close'].shift(1); df['hml']=df['high']-df['low']; df['hmpc']=np.abs(df['high']-df['prev_close']); df['lmpc']=np.abs(df['low']-df['prev_close'])\n",
    "    df['tr']=df[['hml','hmpc','lmpc']].max(axis=1)\n",
    "    atr_periods_min = [14*60, 24*60, 48*60]; min_p_atr = 20\n",
    "    for p in atr_periods_min: df[f'atr_{p}m'] = df['tr'].rolling(p, min_periods=max(min_p_atr, p//4)).mean()\n",
    "    df = df.drop(columns=['prev_close', 'hml', 'hmpc', 'lmpc', 'tr'])\n",
    "    epsilon = 1e-9\n",
    "    for p in [24*60, 168*60]: mc=f'ma_{p}m'; df[f'close_div_ma_{p}m'] = df['close']/(df[mc]+epsilon) if mc in df else np.nan\n",
    "    if 'ma_720m' in df and 'ma_2880m' in df: df['ma720_div_ma2880'] = df['ma_720m']/(df['ma_2880m']+epsilon)\n",
    "    else: df['ma720_div_ma2880']=np.nan\n",
    "    if 'ma_1440m' in df and 'ma_10080m' in df: df['ma1440_div_ma10080'] = df['ma_1440m']/(df['ma_10080m']+epsilon)\n",
    "    else: df['ma1440_div_ma10080']=np.nan\n",
    "    if 'rolling_std_720m' in df and 'rolling_std_4320m' in df: df['std720_div_std4320'] = df['rolling_std_720m']/(df['rolling_std_4320m']+epsilon)\n",
    "    else: df['std720_div_std4320']=np.nan\n",
    "    if 'price_range_pct' in df: df['volumefrom_x_range'] = df['volumefrom'] * df['price_range_pct']\n",
    "    else: df['volumefrom_x_range']=np.nan\n",
    "    if 'rolling_std_180m' in df: df['rolling_std_180m_sq'] = df['rolling_std_180m']**2\n",
    "    else: df['rolling_std_180m_sq']=np.nan\n",
    "    if 'price_change_1m_temp' in df: df['price_return_1m_sq'] = df['price_change_1m_temp']**2 * 10000\n",
    "    else: df['price_return_1m_sq']=np.nan\n",
    "    if 'rolling_std_720m' in df: df['rolling_std_720m_sqrt'] = np.sqrt(df['rolling_std_720m'].clip(lower=0)+epsilon)\n",
    "    else: df['rolling_std_720m_sqrt']=np.nan\n",
    "    cols_to_drop_intermediate = ['price_change_1m_temp', 'volume_return_1m']\n",
    "    df = df.drop(columns=[col for col in cols_to_drop_intermediate if col in df.columns])\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Optuna Objective Function ---\n",
    "# ==============================================================================\n",
    "\n",
    "def objective(trial, X_train, y_train, fixed_params, n_folds):\n",
    "    \"\"\"Objective function for Optuna hyperparameter optimization.\"\"\"\n",
    "\n",
    "    # Define the search space for this trial\n",
    "    param_suggestions = {\n",
    "        # Using suggest_float for continuous params allows finer exploration\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.15, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        # Suggest scale_pos_weight based on potential imbalance.\n",
    "        # If balanced, could fix this to 1. For potentially imbalanced, explore.\n",
    "        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 5),\n",
    "        # Can add back other parameters like reg_alpha, reg_lambda, gamma here if needed\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0, log=True),\n",
    "        # 'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "    }\n",
    "\n",
    "    # Merge fixed and suggested parameters\n",
    "    params = {**fixed_params, **param_suggestions}\n",
    "\n",
    "    cv_scores = []\n",
    "    # Use StratifiedKFold for cross-validation within the objective\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=fixed_params.get('random_state', 42))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        try:\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_tr, y_tr, verbose=False)\n",
    "            preds_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "            # --- Evaluate based on F1 score at a fixed threshold (e.g., 0.5) ---\n",
    "            # This provides a consistent score for Optuna to optimize.\n",
    "            # The final model will still undergo PTT later.\n",
    "            preds_binary = (preds_proba >= 0.5).astype(int)\n",
    "            f1 = f1_score(y_val, preds_binary, zero_division=0)\n",
    "            cv_scores.append(f1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Optuna CV fold {fold} for trial {trial.number}: {e}\")\n",
    "            # Penalize failed trials by returning a very low score\n",
    "            return 0.0 # Or handle as appropriate, e.g., np.nan and let Optuna handle\n",
    "\n",
    "    # Return the average F1 score across folds\n",
    "    average_f1 = np.mean(cv_scores) if cv_scores else 0.0\n",
    "    return average_f1\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Main Script Logic ---\n",
    "# ==============================================================================\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"--- Data Loading ---\")\n",
    "print(f\"Loading data from: {CSV_FILE_PATH} (expecting SOL data without 'symbol' column)\")\n",
    "try:\n",
    "    df_data = pd.read_csv(CSV_FILE_PATH, parse_dates=['timestamp'])\n",
    "    df_data = df_data.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df_data)} rows for {SYMBOL_NAME}.\")\n",
    "except FileNotFoundError: print(f\"Error: {CSV_FILE_PATH} not found.\"); exit()\n",
    "except KeyError as e:\n",
    "    if 'timestamp' in str(e): print(f\"Error: 'timestamp' column not found in {CSV_FILE_PATH}. Please ensure it exists.\"); exit()\n",
    "    else: print(f\"Error loading data: {e}\"); exit()\n",
    "except Exception as e: print(f\"Error loading data: {e}\"); exit()\n",
    "\n",
    "\n",
    "# --- Check if data is sufficient ---\n",
    "if len(df_data) < TRAIN_WINDOW_MINUTES + STEP_MINUTES:\n",
    "    print(f\"Error: Insufficient data for {SYMBOL_NAME} ({len(df_data)} rows) for the initial training window ({TRAIN_WINDOW_MINUTES}) + step ({STEP_MINUTES}). Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(f\"\\n--- Feature Engineering for {SYMBOL_NAME} ---\")\n",
    "start_fe = time.time()\n",
    "df_data = calculate_features_min(df_data)\n",
    "if df_data.empty: print(f\"Error: Feature calculation resulted in empty DataFrame for {SYMBOL_NAME}. Exiting.\"); exit()\n",
    "print(f\"Feature engineering for {SYMBOL_NAME} complete. Took {time.time() - start_fe:.2f} seconds. Columns: {df_data.shape[1]}\")\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"\\n--- Target Definition ---\")\n",
    "print(f\"Defining target as {PREDICTION_WINDOW_MINUTES}m future return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "target_col = f'target_return_{PREDICTION_WINDOW_MINUTES}m'\n",
    "df_data[target_col] = df_data['close'].shift(-PREDICTION_WINDOW_MINUTES).sub(df_data['close']).div(df_data['close'].replace(0, np.nan)).mul(100)\n",
    "\n",
    "# --- 4. Prepare Data for Modeling ---\n",
    "print(\"\\n--- Data Preparation ---\")\n",
    "base_cols_ohlcv = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']\n",
    "cols_to_keep_final = ['timestamp', target_col]\n",
    "potential_feature_cols = [col for col in df_data.columns if col not in cols_to_keep_final and col not in base_cols_ohlcv]\n",
    "numeric_feature_cols = df_data[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "final_feature_cols = numeric_feature_cols\n",
    "cols_to_select = final_feature_cols + [col for col in cols_to_keep_final if col in df_data.columns]\n",
    "df_model_ready = df_data[cols_to_select].copy()\n",
    "\n",
    "# --- NaN / Inf Handling ---\n",
    "print(\"Applying NaN/Inf Handling...\")\n",
    "initial_cols = len(final_feature_cols)\n",
    "nan_threshold = 0.3\n",
    "nan_percentages = df_model_ready[final_feature_cols].isna().mean()\n",
    "cols_to_drop = nan_percentages[nan_percentages > nan_threshold].index.tolist()\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >{nan_threshold*100:.0f}% NaN values: {cols_to_drop}\")\n",
    "    df_model_ready = df_model_ready.drop(columns=cols_to_drop)\n",
    "    final_feature_cols = [col for col in final_feature_cols if col not in cols_to_drop]\n",
    "initial_rows = len(df_model_ready)\n",
    "df_model_ready = df_model_ready.dropna(subset=final_feature_cols + [target_col])\n",
    "final_rows = len(df_model_ready)\n",
    "print(f\"NaN Handling: Dropped {len(cols_to_drop)} columns. Dropped {initial_rows - final_rows} rows with remaining NaNs.\")\n",
    "if not final_feature_cols: print(f\"Error: No numeric feature columns remaining after NaN handling for {SYMBOL_NAME}. Exiting.\"); exit()\n",
    "numeric_cols_final_check = df_model_ready[final_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "inf_mask = np.isinf(df_model_ready[numeric_cols_final_check]); inf_count = inf_mask.sum().sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"Replacing {inf_count} infinites with NaN in features...\")\n",
    "    df_model_ready.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    rows_b4 = len(df_model_ready)\n",
    "    df_model_ready = df_model_ready.dropna(subset=final_feature_cols)\n",
    "    print(f\"Dropped {rows_b4 - len(df_model_ready)} more rows after Inf handling.\")\n",
    "if df_model_ready.empty: print(f\"Error: DataFrame empty after NaN/Inf handling for {SYMBOL_NAME}. Exiting.\"); exit()\n",
    "\n",
    "# --- Final X, y, timestamps ---\n",
    "X = df_model_ready[final_feature_cols]\n",
    "y_binary = (df_model_ready[target_col] >= TARGET_THRESHOLD_PCT).astype(int)\n",
    "timestamps = df_model_ready['timestamp']\n",
    "print(f\"\\nFinal feature matrix shape for {SYMBOL_NAME}: {X.shape}\")\n",
    "print(f\"Target vector shape for {SYMBOL_NAME}: {y_binary.shape}\")\n",
    "print(f\"Using {len(final_feature_cols)} features.\")\n",
    "\n",
    "# --- 5. SLIDING Window Backtesting (with Optuna) ---\n",
    "print(f\"\\n--- Starting SLIDING Window Backtest for {SYMBOL_NAME} with Optuna ({N_OPTUNA_TRIALS} trials/step) ---\")\n",
    "if len(X) < TRAIN_WINDOW_MINUTES + STEP_MINUTES:\n",
    "    print(f\"Error: Not enough data for {SYMBOL_NAME} ({len(X)}) after pre-processing for train window ({TRAIN_WINDOW_MINUTES}) + step ({STEP_MINUTES}). Exiting.\")\n",
    "    exit()\n",
    "\n",
    "all_predictions_proba = []; all_actual = []; backtest_timestamps = []\n",
    "all_best_params = [] # Store best params found by Optuna for each step\n",
    "num_steps = 0\n",
    "start_index_loop = TRAIN_WINDOW_MINUTES\n",
    "end_index_loop = len(X) - TEST_WINDOW_MINUTES + 1\n",
    "\n",
    "print(f\"Train Window: {TRAIN_WINDOW_MINUTES} mins, Step: {STEP_MINUTES} mins, Test Window: {TEST_WINDOW_MINUTES} mins\")\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for i in range(start_index_loop, end_index_loop, STEP_MINUTES):\n",
    "    step_start_time = time.time()\n",
    "    train_idx_start = i - TRAIN_WINDOW_MINUTES\n",
    "    train_idx_end = i\n",
    "    test_idx_start = i\n",
    "    test_idx_end = min(i + TEST_WINDOW_MINUTES, len(X))\n",
    "\n",
    "    if test_idx_start >= test_idx_end: print(f\"Stopping loop: Test window invalid.\"); break\n",
    "\n",
    "    X_train_roll = X.iloc[train_idx_start : train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[train_idx_start : train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx_start : test_idx_end]\n",
    "    y_test_roll_actual_series = y_binary.iloc[test_idx_start : test_idx_end]\n",
    "    step_timestamps = timestamps.iloc[test_idx_start : test_idx_end]\n",
    "\n",
    "    if y_test_roll_actual_series.empty: print(f\"Warning: Skipping step {i} for {SYMBOL_NAME}. Empty test actuals.\"); continue\n",
    "    current_timestamp = step_timestamps.iloc[0]\n",
    "    if X_train_roll.empty or len(np.unique(y_train_roll)) < 2:\n",
    "        print(f\"Warning: Skipping step {i} for {SYMBOL_NAME}. Invalid training data.\"); continue\n",
    "\n",
    "    print(f\"\\n--- {SYMBOL_NAME} - Step {num_steps + 1} (Predicting window starting {current_timestamp}) ---\")\n",
    "    print(f\"  Training indices: [{train_idx_start}:{train_idx_end-1}]; Testing indices: [{test_idx_start}:{test_idx_end-1}]\")\n",
    "\n",
    "    # --- Hyperparameter Tuning with Optuna ---\n",
    "    print(f\"  Running Optuna ({N_OPTUNA_TRIALS} trials, {N_CV_FOLDS}-fold CV, maximizing F1@0.5)...\")\n",
    "    optuna_start_time = time.time()\n",
    "    try:\n",
    "        # Create study object. Maximize F1 score. Add pruner.\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5) # Prune after 5 trials\n",
    "        )\n",
    "\n",
    "        # Define the objective function with necessary arguments for this step\n",
    "        obj_func = lambda trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, N_CV_FOLDS)\n",
    "\n",
    "        # Run optimization\n",
    "        study.optimize(obj_func, n_trials=N_OPTUNA_TRIALS, timeout=None) # Set timeout (seconds) if needed\n",
    "\n",
    "        best_params_step = study.best_params\n",
    "        best_score_step = study.best_value # Best average F1@0.5 found in CV\n",
    "        print(f\"  Optuna finished in {time.time() - optuna_start_time:.2f}s.\")\n",
    "        print(f\"  Best Params (suggested): {best_params_step}\")\n",
    "        print(f\"  Best CV F1 Score (at 0.5 thresh): {best_score_step:.4f}\")\n",
    "        all_best_params.append({'step': num_steps + 1, 'params': best_params_step, 'cv_f1': best_score_step})\n",
    "\n",
    "        # --- Fit final model for the step using best params found by Optuna ---\n",
    "        final_model_params = {**XGB_FIXED_PARAMS, **best_params_step} # Combine fixed and best suggested\n",
    "        model_roll = xgb.XGBClassifier(**final_model_params)\n",
    "        model_roll.fit(X_train_roll, y_train_roll, verbose=False)\n",
    "\n",
    "        # --- Predict probabilities for the test window ---\n",
    "        prob_roll_window = model_roll.predict_proba(X_test_roll)[:, 1]\n",
    "\n",
    "        # --- Store results ---\n",
    "        all_predictions_proba.extend(prob_roll_window)\n",
    "        all_actual.extend(y_test_roll_actual_series.tolist())\n",
    "        backtest_timestamps.extend(step_timestamps.tolist())\n",
    "        num_steps += 1\n",
    "\n",
    "    except optuna.exceptions.TrialPruned as e:\n",
    "         print(f\"!! Optuna trial pruned at step {i}: {e}\")\n",
    "         # Decide how to handle: continue to next step or stop? Let's continue.\n",
    "         continue\n",
    "    except Exception as e_step:\n",
    "        print(f\"!! Error during Optuna study or final fit at step {i} for {SYMBOL_NAME}: {e_step}\")\n",
    "        traceback.print_exc() # Print stack trace for debugging\n",
    "        continue # Skip to next step\n",
    "\n",
    "    step_end_time = time.time()\n",
    "    print(f\"  Step {num_steps} finished in {step_end_time - step_start_time:.2f}s total.\")\n",
    "\n",
    "\n",
    "loop_end_time = time.time()\n",
    "print(f\"\\nBacktesting loop for {SYMBOL_NAME} finished. Completed {num_steps} steps (each predicting up to {TEST_WINDOW_MINUTES} points) in {(loop_end_time - loop_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results with PTT (Unchanged) ---\n",
    "if num_steps > 0 and len(all_predictions_proba) == len(all_actual):\n",
    "    print(f\"\\n--- Evaluating Results for {SYMBOL_NAME} with Probability Threshold Tuning ---\")\n",
    "    print(f\"Threshold search range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    best_threshold = 0.5; best_f1_thresh = -1.0\n",
    "    results_per_threshold = {}\n",
    "    probabilities_np = np.array(all_predictions_proba)\n",
    "    actual_np = np.array(all_actual)\n",
    "\n",
    "    for t in THRESHOLD_SEARCH_RANGE:\n",
    "        predictions_thresh = (probabilities_np >= t).astype(int)\n",
    "        if np.sum(actual_np) == 0 and np.sum(predictions_thresh) == 0: acc_t, pre_t, rec_t, f1_t = 1.0, 1.0, 1.0, 1.0\n",
    "        elif np.sum(actual_np) > 0 and np.sum(predictions_thresh) == 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        elif np.sum(actual_np) == 0 and np.sum(predictions_thresh) > 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh)\n",
    "             pre_t = precision_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             rec_t = recall_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             f1_t = f1_score(actual_np, predictions_thresh, zero_division=0)\n",
    "        results_per_threshold[round(t, 2)] = {'f1': f1_t, 'acc': acc_t, 'pre': pre_t, 'rec': rec_t}\n",
    "        if f1_t >= best_f1_thresh:\n",
    "             if f1_t > best_f1_thresh or abs(t - 0.5) < abs(best_threshold - 0.5):\n",
    "                  best_f1_thresh = f1_t; best_threshold = t\n",
    "\n",
    "    print(f\"\\nBest Threshold for {SYMBOL_NAME} found: {best_threshold:.2f} (Yielding F1 Score: {best_f1_thresh:.4f})\")\n",
    "    final_predictions_optimized = (probabilities_np >= best_threshold).astype(int)\n",
    "    final_accuracy = accuracy_score(actual_np, final_predictions_optimized)\n",
    "    final_precision = precision_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_recall = recall_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_f1 = f1_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "\n",
    "    print(f\"\\n--- Final Performance Metrics for {SYMBOL_NAME} (Optimized Threshold) ---\")\n",
    "    print(f\"Target: {PREDICTION_WINDOW_MINUTES}m return >= {TARGET_THRESHOLD_PCT}%\")\n",
    "    print(f\"Windowing: Train={TRAIN_WINDOW_MINUTES}m, Step={STEP_MINUTES}m, Test Window={TEST_WINDOW_MINUTES}m\")\n",
    "    print(f\"Hyperparameter Tuning: Optuna ({N_OPTUNA_TRIALS} trials/step)\") # Added Optuna info\n",
    "    print(f\"Total Individual Predictions Evaluated: {len(actual_np)}\")\n",
    "    print(f\"Positive Target Occurrence: {actual_np.mean()*100:.2f}%\")\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "    if 0.5 in results_per_threshold:\n",
    "        res_def = results_per_threshold[0.5]\n",
    "        print(f\"(Compare: Default 0.5 Thresh -> F1:{res_def['f1']:.4f}, Acc:{res_def['acc']:.4f}, Pre:{res_def['pre']:.4f}, Rec:{res_def['rec']:.4f})\")\n",
    "\n",
    "    results_summary = { # Store results (unchanged)\n",
    "        'symbol': SYMBOL_NAME,'probabilities': probabilities_np,'actuals': actual_np,'timestamps': backtest_timestamps,\n",
    "        'best_threshold': best_threshold,'metrics_optimized': {'acc': final_accuracy, 'pre': final_precision, 'rec': final_recall, 'f1': final_f1},\n",
    "        'metrics_default_0.5': results_per_threshold.get(0.5, {}),'best_params_per_step': all_best_params,'results_per_threshold': results_per_threshold\n",
    "    }\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy (Unchanged) ---\n",
    "    print(f\"\\nPlotting cumulative accuracy for {SYMBOL_NAME} (optimized threshold)...\")\n",
    "    if len(backtest_timestamps) != len(actual_np):\n",
    "         print(f\"Warning: Timestamp length ({len(backtest_timestamps)}) mismatch with prediction length ({len(actual_np)}). Skipping plot.\")\n",
    "    else:\n",
    "        try:\n",
    "            cumulative_accuracy_list_optimized = (np.cumsum(final_predictions_optimized == actual_np) / np.arange(1, len(actual_np) + 1))\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.plot(backtest_timestamps, cumulative_accuracy_list_optimized, marker='.', linestyle='-', markersize=1, alpha=0.7, label=f'Cumulative Accuracy ({SYMBOL_NAME})')\n",
    "            rolling_window_plot_size = max(TEST_WINDOW_MINUTES * 5, 60 * 12)\n",
    "            if len(actual_np) > rolling_window_plot_size:\n",
    "                 results_df = pd.DataFrame({'correct': (final_predictions_optimized == actual_np).astype(int)}, index=pd.to_datetime(backtest_timestamps))\n",
    "                 try: rolling_acc = results_df['correct'].rolling(window=rolling_window_plot_size, min_periods=rolling_window_plot_size//2).mean()\n",
    "                 except Exception as e_roll: print(f\"Could not calculate rolling accuracy: {e_roll}\"); rolling_acc = None\n",
    "                 if rolling_acc is not None: plt.plot(rolling_acc.index, rolling_acc, linestyle='--', color='red', label=f'Rolling Acc ({rolling_window_plot_size} min window)')\n",
    "            plt.title(f'{SYMBOL_NAME} Backtest (Optuna Tune, Train:{TRAIN_WINDOW_MINUTES}m, Step:{STEP_MINUTES}m) - Best Thresh: {best_threshold:.2f}') # Updated title\n",
    "            plt.xlabel('Timestamp'); plt.ylabel('Accuracy')\n",
    "            min_y_plot = max(0.0, np.min(cumulative_accuracy_list_optimized) - 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.4)\n",
    "            max_y_plot = min(1.0, np.max(cumulative_accuracy_list_optimized) + 0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.8)\n",
    "            if max_y_plot - min_y_plot < 0.1: mid_point=(max_y_plot+min_y_plot)/2; min_y_plot=max(0.0, mid_point-0.05); max_y_plot=min(1.0, mid_point+0.05)\n",
    "            plt.ylim(min_y_plot, max_y_plot)\n",
    "            plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "            plot_filename = f\"backtest_accuracy_{SYMBOL_NAME}_12h_target_optuna.png\" # Updated filename\n",
    "            plt.savefig(plot_filename); print(f\"Saved accuracy plot to {plot_filename}\"); plt.close()\n",
    "        except Exception as e_plot: print(f\"Error plotting for {SYMBOL_NAME}: {e_plot}\")\n",
    "else:\n",
    "    print(f\"No predictions were made/stored for {SYMBOL_NAME}, cannot evaluate or plot.\")\n",
    "\n",
    "# --- End of Script ---\n",
    "print(f\"\\n{'='*30} Overall Script Finished for {SYMBOL_NAME} {'='*30}\")\n",
    "overall_end_time = time.time()\n",
    "print(f\"Total execution time: {(overall_end_time - overall_start_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD',\n",
       "       'price_range_pct', 'oc_change_pct', 'garman_klass_12h', 'parkinson_3h',\n",
       "       'ma_3h', 'rolling_std_3h', 'lag_3h_price_return', 'lag_6h_price_return',\n",
       "       'lag_12h_price_return', 'lag_24h_price_return', 'lag_48h_price_return',\n",
       "       'lag_72h_price_return', 'lag_168h_price_return', 'volume_return_1h',\n",
       "       'lag_3h_volume_return', 'lag_6h_volume_return', 'lag_12h_volume_return',\n",
       "       'lag_24h_volume_return', 'ma_6h', 'ma_12h', 'ma_24h', 'ma_48h',\n",
       "       'ma_72h', 'ma_168h', 'rolling_std_6h', 'rolling_std_12h',\n",
       "       'rolling_std_24h', 'rolling_std_48h', 'rolling_std_72h',\n",
       "       'rolling_std_168h', 'atr_14h', 'atr_24h', 'atr_48h', 'close_div_ma_24h',\n",
       "       'close_div_ma_48h', 'close_div_ma_168h', 'ma12_div_ma48',\n",
       "       'ma24_div_ma168', 'std12_div_std72', 'volume_btc_x_range',\n",
       "       'rolling_std_3h_sq', 'price_return_1h_sq', 'rolling_std_12h_sqrt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
