{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Loading ---\n",
      "Loading data from: SOL_minagg.csv...\n",
      "Loaded 22582 rows for SOL.\n",
      "\n",
      "--- Feature Engineering for SOL ---\n",
      "Feature engineering complete. Took 0.07s.\n",
      "\n",
      "--- Target Definition ---\n",
      "Defining target: 60m return >= 0.24%...\n",
      "  Raw positive target occurrence (before NaN drop): 38.62%\n",
      "\n",
      "--- Data Preparation ---\n",
      "Applying final NaN/Inf Handling...\n",
      "NaN Handling: Dropped 121 rows with NaNs.\n",
      "\n",
      "Final feature matrix shape: (22461, 46), Target shape: (22461,)\n",
      "Using 46 features.\n",
      "Positive Target Rate in Final Data: 38.70%\n",
      "\n",
      "--- Starting SLIDING Window Backtest for SOL ---\n",
      "!!! Using Optuna (TimeSeriesSplit CV + SMOTE, optimizing Precision) + Rare Event Features !!!\n",
      "Train Window: 480m, Step: 240m, Test Window: 96m, Optuna Trials: 50, CV Splits: 3\n",
      "\n",
      "--- Step 1 (2025-04-03 10:01:00) ---\n",
      "  Train: [0:479]; Test: [480:575]\n",
      "  Training data balance before SMOTE: 19.38% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 31.72s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 2, 'reg_alpha': 0.00872793843526137, 'reg_lambda': 1.352830811024157, 'gamma': 0.2335067318901289, 'subsample': 0.7461886502831545, 'colsample_bytree': 0.8246950356742114, 'learning_rate': 0.12101275716333965}, Best CV Precision(@0.25): 0.1822\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 1 finished in 31.95s total.\n",
      "\n",
      "--- Step 2 (2025-04-03 14:01:00) ---\n",
      "  Train: [240:719]; Test: [720:815]\n",
      "  Training data balance before SMOTE: 14.79% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 33.78s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 1, 'reg_alpha': 0.0039574326788431495, 'reg_lambda': 5.62519484745887, 'gamma': 0.23571605753896818, 'subsample': 0.7820489519762077, 'colsample_bytree': 0.7962665551280764, 'learning_rate': 0.054430551723220755}, Best CV Precision(@0.25): 0.3400\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 2 finished in 34.09s total.\n",
      "\n",
      "--- Step 3 (2025-04-03 18:01:00) ---\n",
      "  Train: [480:959]; Test: [960:1055]\n",
      "  Training data balance before SMOTE: 33.54% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 28.14s.\n",
      "  Best Params: {'max_depth': 8, 'min_child_weight': 4, 'reg_alpha': 0.039330265394998565, 'reg_lambda': 3.716496694463903, 'gamma': 0.32884348291126364, 'subsample': 0.8147501850302797, 'colsample_bytree': 0.8596552691911518, 'learning_rate': 0.03482224517196455}, Best CV Precision(@0.25): 0.5858\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 3 finished in 28.40s total.\n",
      "\n",
      "--- Step 4 (2025-04-03 22:01:00) ---\n",
      "  Train: [720:1199]; Test: [1200:1295]\n",
      "  Training data balance before SMOTE: 62.08% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 31.99s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 1, 'reg_alpha': 0.001586700627750826, 'reg_lambda': 1.7134662483225007, 'gamma': 0.005446197700750499, 'subsample': 0.9236476928327942, 'colsample_bytree': 0.8287258898617993, 'learning_rate': 0.13805299166251622}, Best CV Precision(@0.25): 0.9552\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 4 finished in 32.33s total.\n",
      "\n",
      "--- Step 5 (2025-04-04 02:01:00) ---\n",
      "  Train: [960:1439]; Test: [1440:1535]\n",
      "  Training data balance before SMOTE: 45.21% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 34.43s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 1, 'reg_alpha': 0.001031696512434844, 'reg_lambda': 1.1939926724930887, 'gamma': 0.19728945624035754, 'subsample': 0.7402465986329814, 'colsample_bytree': 0.700625535166722, 'learning_rate': 0.08010511695491102}, Best CV Precision(@0.25): 0.4724\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 5 finished in 34.74s total.\n",
      "\n",
      "--- Step 6 (2025-04-04 06:01:00) ---\n",
      "  Train: [1200:1679]; Test: [1680:1775]\n",
      "  Training data balance before SMOTE: 28.12% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 32.15s.\n",
      "  Best Params: {'max_depth': 8, 'min_child_weight': 7, 'reg_alpha': 0.0024013193683141395, 'reg_lambda': 5.531043190101432, 'gamma': 0.1767366390801286, 'subsample': 0.7012932450198058, 'colsample_bytree': 0.744023753326033, 'learning_rate': 0.03716249456546076}, Best CV Precision(@0.25): 0.3389\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 6 finished in 32.35s total.\n",
      "\n",
      "--- Step 7 (2025-04-04 10:01:00) ---\n",
      "  Train: [1440:1919]; Test: [1920:2015]\n",
      "  Training data balance before SMOTE: 51.88% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 26.84s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 1, 'reg_alpha': 0.005499182611553082, 'reg_lambda': 3.223039571506211, 'gamma': 0.19660353789064575, 'subsample': 0.978867262962164, 'colsample_bytree': 0.8091588453974903, 'learning_rate': 0.0854212123685425}, Best CV Precision(@0.25): 0.4722\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 7 finished in 27.11s total.\n",
      "\n",
      "--- Step 8 (2025-04-04 14:01:00) ---\n",
      "  Train: [1680:2159]; Test: [2160:2255]\n",
      "  Training data balance before SMOTE: 58.33% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 35.62s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 2, 'reg_alpha': 0.001226252539309605, 'reg_lambda': 1.124851028785278, 'gamma': 0.4536171767952289, 'subsample': 0.9992230643246676, 'colsample_bytree': 0.8415794713556662, 'learning_rate': 0.03899017165531209}, Best CV Precision(@0.25): 0.5490\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 8 finished in 35.99s total.\n",
      "\n",
      "--- Step 9 (2025-04-04 18:01:00) ---\n",
      "  Train: [1920:2399]; Test: [2400:2495]\n",
      "  Training data balance before SMOTE: 61.25% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 32.53s.\n",
      "  Best Params: {'max_depth': 8, 'min_child_weight': 1, 'reg_alpha': 0.02480558949301645, 'reg_lambda': 2.768262804442701, 'gamma': 0.44424232504135647, 'subsample': 0.7479586578327008, 'colsample_bytree': 0.8502262829162063, 'learning_rate': 0.04029541627386252}, Best CV Precision(@0.25): 0.9583\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 9 finished in 32.82s total.\n",
      "\n",
      "--- Step 10 (2025-04-04 22:01:00) ---\n",
      "  Train: [2160:2639]; Test: [2640:2735]\n",
      "  Training data balance before SMOTE: 62.71% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 27.84s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 5, 'reg_alpha': 0.016463260280751728, 'reg_lambda': 2.0429420280952932, 'gamma': 0.28351398178882103, 'subsample': 0.7474391889168899, 'colsample_bytree': 0.810771360404323, 'learning_rate': 0.1376876166936459}, Best CV Precision(@0.25): 0.6866\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 10 finished in 28.06s total.\n",
      "\n",
      "--- Step 11 (2025-04-05 02:01:00) ---\n",
      "  Train: [2400:2879]; Test: [2880:2975]\n",
      "  Training data balance before SMOTE: 29.38% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 31.12s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 4, 'reg_alpha': 0.002259314265176789, 'reg_lambda': 2.0152958162508017, 'gamma': 0.02600456948526325, 'subsample': 0.9111924919304857, 'colsample_bytree': 0.891056611977788, 'learning_rate': 0.10308474874922992}, Best CV Precision(@0.25): 0.4253\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 11 finished in 31.38s total.\n",
      "\n",
      "--- Step 12 (2025-04-05 06:01:00) ---\n",
      "  Train: [2640:3119]; Test: [3120:3215]\n",
      "  Training data balance before SMOTE: 6.67% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 21.79s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 5, 'reg_alpha': 0.008548134952014013, 'reg_lambda': 1.1573026356176959, 'gamma': 0.24561097529332104, 'subsample': 0.9092414396638785, 'colsample_bytree': 0.7204595772321998, 'learning_rate': 0.0786604768628691}, Best CV Precision(@0.25): 0.0538\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 12 finished in 22.01s total.\n",
      "\n",
      "--- Step 13 (2025-04-05 10:01:00) ---\n",
      "  Train: [2880:3359]; Test: [3360:3455]\n",
      "  Training data balance before SMOTE: 26.25% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 32.87s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 1, 'reg_alpha': 0.0020495956683694214, 'reg_lambda': 6.623521147676188, 'gamma': 0.04401634481650363, 'subsample': 0.9876216683628889, 'colsample_bytree': 0.7094371534645557, 'learning_rate': 0.08999617132988674}, Best CV Precision(@0.25): 0.6667\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 13 finished in 33.21s total.\n",
      "\n",
      "--- Step 14 (2025-04-05 14:01:00) ---\n",
      "  Train: [3120:3599]; Test: [3600:3695]\n",
      "  Training data balance before SMOTE: 26.46% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 31.27s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 4, 'reg_alpha': 0.2387010017123245, 'reg_lambda': 1.3409539120992975, 'gamma': 0.4414378669466598, 'subsample': 0.7862587345560287, 'colsample_bytree': 0.7850492834856939, 'learning_rate': 0.07182157308329021}, Best CV Precision(@0.25): 0.3448\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 14 finished in 31.54s total.\n",
      "\n",
      "--- Step 15 (2025-04-05 18:01:00) ---\n",
      "  Train: [3360:3839]; Test: [3840:3935]\n",
      "  Training data balance before SMOTE: 23.54% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 18.73s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 5, 'reg_alpha': 0.05092800498371158, 'reg_lambda': 9.859705696115348, 'gamma': 0.3535390006682987, 'subsample': 0.7934085828350192, 'colsample_bytree': 0.6027150529582886, 'learning_rate': 0.1093279135974745}, Best CV Precision(@0.25): 0.2694\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 15 finished in 18.96s total.\n",
      "\n",
      "--- Step 16 (2025-04-05 22:01:00) ---\n",
      "  Train: [3600:4079]; Test: [4080:4175]\n",
      "  Training data balance before SMOTE: 36.88% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 33.91s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 5, 'reg_alpha': 0.016194370101147087, 'reg_lambda': 2.7101631498749668, 'gamma': 0.2083844026355798, 'subsample': 0.9904466459631636, 'colsample_bytree': 0.8868071549131128, 'learning_rate': 0.03316839710317675}, Best CV Precision(@0.25): 0.5456\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 16 finished in 34.23s total.\n",
      "\n",
      "--- Step 17 (2025-04-06 02:01:00) ---\n",
      "  Train: [3840:4319]; Test: [4320:4415]\n",
      "  Training data balance before SMOTE: 39.38% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 33.67s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 2, 'reg_alpha': 0.4965928672601835, 'reg_lambda': 5.207177523926186, 'gamma': 0.01622327731925557, 'subsample': 0.9249687496417895, 'colsample_bytree': 0.7112064917902343, 'learning_rate': 0.030371519514613914}, Best CV Precision(@0.25): 0.4782\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 17 finished in 34.00s total.\n",
      "\n",
      "--- Step 18 (2025-04-06 06:01:00) ---\n",
      "  Train: [4080:4559]; Test: [4560:4655]\n",
      "  Training data balance before SMOTE: 32.71% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 34.77s.\n",
      "  Best Params: {'max_depth': 7, 'min_child_weight': 1, 'reg_alpha': 0.0068663580299965655, 'reg_lambda': 6.966793006506084, 'gamma': 0.08087191350789272, 'subsample': 0.8543802993045877, 'colsample_bytree': 0.8196945090413574, 'learning_rate': 0.06676111721360277}, Best CV Precision(@0.25): 0.2760\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 18 finished in 35.07s total.\n",
      "\n",
      "--- Step 19 (2025-04-06 10:01:00) ---\n",
      "  Train: [4320:4799]; Test: [4800:4895]\n",
      "  Training data balance before SMOTE: 20.42% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 32.39s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 1, 'reg_alpha': 0.110621993395228, 'reg_lambda': 1.753043557088147, 'gamma': 0.4999461644026809, 'subsample': 0.8557459999677397, 'colsample_bytree': 0.8354315432269206, 'learning_rate': 0.07761965360840002}, Best CV Precision(@0.25): 0.5000\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 19 finished in 32.61s total.\n",
      "\n",
      "--- Step 20 (2025-04-06 14:01:00) ---\n",
      "  Train: [4560:5039]; Test: [5040:5135]\n",
      "  Training data balance before SMOTE: 13.12% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 25.95s.\n",
      "  Best Params: {'max_depth': 10, 'min_child_weight': 1, 'reg_alpha': 0.005108905798690752, 'reg_lambda': 2.7872146746034105, 'gamma': 0.43831016905273507, 'subsample': 0.7259131092645341, 'colsample_bytree': 0.6894458458417347, 'learning_rate': 0.08409160520772488}, Best CV Precision(@0.25): 0.0000\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 20 finished in 26.20s total.\n",
      "\n",
      "--- Step 21 (2025-04-06 18:01:00) ---\n",
      "  Train: [4800:5279]; Test: [5280:5375]\n",
      "  Training data balance before SMOTE: 7.71% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 19.06s.\n",
      "  Best Params: {'max_depth': 5, 'min_child_weight': 6, 'reg_alpha': 0.008113369293098565, 'reg_lambda': 3.5750803013150025, 'gamma': 0.16169989793142422, 'subsample': 0.9562839030487594, 'colsample_bytree': 0.880698770110209, 'learning_rate': 0.1284438895566442}, Best CV Precision(@0.25): 0.1296\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 21 finished in 19.24s total.\n",
      "\n",
      "--- Step 22 (2025-04-06 22:01:00) ---\n",
      "  Train: [5040:5519]; Test: [5520:5615]\n",
      "  Training data balance before SMOTE: 16.04% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 32.22s.\n",
      "  Best Params: {'max_depth': 9, 'min_child_weight': 5, 'reg_alpha': 0.0012071459247680522, 'reg_lambda': 1.0064803351718419, 'gamma': 0.09207302528883365, 'subsample': 0.8817954094125308, 'colsample_bytree': 0.7685397989535031, 'learning_rate': 0.1144679849120302}, Best CV Precision(@0.25): 0.2361\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 22 finished in 32.42s total.\n",
      "\n",
      "--- Step 23 (2025-04-07 02:01:00) ---\n",
      "  Train: [5280:5759]; Test: [5760:5855]\n",
      "  Training data balance before SMOTE: 28.75% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n",
      "  Optuna finished in 33.75s.\n",
      "  Best Params: {'max_depth': 6, 'min_child_weight': 2, 'reg_alpha': 0.0013296781078158955, 'reg_lambda': 1.490171003609189, 'gamma': 0.11105616688611686, 'subsample': 0.9139367790602844, 'colsample_bytree': 0.7111330776945538, 'learning_rate': 0.0761995778896158}, Best CV Precision(@0.25): 0.5566\n",
      "  Fitting final model on original training data for step...\n",
      "  Step 23 finished in 34.03s total.\n",
      "\n",
      "--- Step 24 (2025-04-07 06:01:00) ---\n",
      "  Train: [5520:5999]; Test: [6000:6095]\n",
      "  Training data balance before SMOTE: 18.96% positive\n",
      "  Running Optuna (50 trials, cv=3 TimeSeriesSplit+SMOTE, scoring Precision@0.25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x00000223087C1220>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 534, in _next_wrapper\n",
      "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 469, in _handle_exception\n",
      "    return fn()\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 534, in <lambda>\n",
      "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py\", line 1185, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 528, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 819, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 950, in set_label\n",
      "    dispatch_meta_backend(self, label, 'label', 'float')\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py\", line 1127, in dispatch_meta_backend\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\mason\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\data.py\", line 1050, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# B3_Configurable_Minute_SOL_Optuna_V6_Precision.py # <-- Renamed\n",
    "# Optuna maximizes Precision@Threshold in CV folds, uses SMOTE & updated features.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas_ta as ta\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Modeling Imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "CSV_FILE_PATH = 'SOL_minagg.csv'; SYMBOL_NAME = 'SOL'\n",
    "PREDICTION_WINDOW_MINUTES = int(1 * 60); TARGET_THRESHOLD_PCT = 0.24 # Predict +2% in 30m\n",
    "TRAIN_WINDOW_MINUTES = 8 * 60; STEP_MINUTES = 4 * 60; TEST_WINDOW_FRACTION = 0.2\n",
    "XGB_FIXED_PARAMS = {\"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"use_label_encoder\": False, \"random_state\":42, \"tree_method\":\"gpu_hist\", \"predictor\":\"gpu_predictor\", \"gpu_id\":0, \"n_jobs\":-1, \"n_estimators\":150}\n",
    "N_OPTUNA_TRIALS = 50; OPTUNA_CV_SPLITS = 3\n",
    "# Optuna will optimize PRECISION calculated at this threshold\n",
    "OPTUNA_EVAL_THRESHOLD = 0.25 # Keep lower threshold for rare positive class eval\n",
    "SMOTE_K_NEIGHBORS = 5\n",
    "PROBABILITY_THRESHOLD_RANGE = (0.05, 0.95); PROBABILITY_THRESHOLD_STEP = 0.05\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Derived Variables ---\n",
    "# ==============================================================================\n",
    "TEST_WINDOW_MINUTES = max(1, int(TEST_WINDOW_FRACTION * TRAIN_WINDOW_MINUTES))\n",
    "THRESHOLD_SEARCH_RANGE = np.arange(PROBABILITY_THRESHOLD_RANGE[0], PROBABILITY_THRESHOLD_RANGE[1], PROBABILITY_THRESHOLD_STEP)\n",
    "epsilon = 1e-9\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Feature Engineering Function (calculate_features_min_rare_event - Unchanged) ---\n",
    "# ==============================================================================\n",
    "def calculate_features_min_rare_event(df_input):\n",
    "    df = df_input.copy()\n",
    "    # print(f\"  Feature Eng Start: Initial rows = {len(df)}\") # Optional debug\n",
    "    essential_cols = ['open', 'high', 'low', 'close', 'volumefrom']\n",
    "    initial_nan_check = df[essential_cols].isnull().sum()\n",
    "    if initial_nan_check.sum() > 0: df = df.dropna(subset=essential_cols)\n",
    "    if df.empty: return df\n",
    "    base_cols_numeric = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']\n",
    "    for col in base_cols_numeric:\n",
    "        if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else: df[col] = 0\n",
    "    df = df.dropna(subset=essential_cols)\n",
    "    if df.empty: return df\n",
    "    df['price_change_1m_temp'] = df['close'].pct_change(periods=1)\n",
    "    df['body_abs'] = abs(df['close'] - df['open'])\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    df['body_ratio'] = (df['body_abs'] / (df['range'] + epsilon)).clip(0, 1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df['price_range_pct'] = (df['range'] / df['close'].replace(0, np.nan)) * 100\n",
    "        df['oc_change_pct'] = (df['close'] - df['open']) / df['open'].replace(0, np.nan) * 100\n",
    "    min_periods_rolling = 2\n",
    "    for p in [10, 30, 60]: df[f'ma_{p}m'] = df['close'].rolling(p, min_periods=min_periods_rolling).mean()\n",
    "    for p in [30, 60]: df[f'rolling_std_{p}m'] = df['price_change_1m_temp'].rolling(p, min_periods=p//2).std() * 100\n",
    "    lag_periods_price_min = [1, 3, 5, 10, 15, 30, 60]; lag_periods_volume_min = [1, 3, 5, 10, 15, 30, 60]\n",
    "    for lag in lag_periods_price_min: df[f'lag_{lag}m_price_return'] = df['price_change_1m_temp'].shift(lag) * 100\n",
    "    df['volume_return_1m'] = df['volumefrom'].pct_change(periods=1).replace([np.inf, -np.inf], 0) * 100\n",
    "    for lag in lag_periods_volume_min: df[f'lag_{lag}m_volume_return'] = df['volume_return_1m'].shift(lag)\n",
    "    vol_ma_period = 20; df[f'vol_ma_{vol_ma_period}m'] = df['volumefrom'].rolling(vol_ma_period, min_periods=vol_ma_period//2).mean()\n",
    "    df['vol_spike_ratio'] = df['volumefrom'] / (df[f'vol_ma_{vol_ma_period}m'] + epsilon)\n",
    "    body_ma_period = 20; df[f'body_ma_{body_ma_period}m'] = df['body_abs'].rolling(body_ma_period, min_periods=body_ma_period//2).mean()\n",
    "    df['body_spike_ratio'] = df['body_abs'] / (df[f'body_ma_{body_ma_period}m'] + epsilon)\n",
    "    df['prev_close']=df['close'].shift(1); df['hml']=df['high']-df['low']; df['hmpc']=np.abs(df['high']-df['prev_close']); df['lmpc']=np.abs(df['low']-df['prev_close'])\n",
    "    df['tr']=df[['hml','hmpc','lmpc']].max(axis=1); atr_periods_min = [14]; min_p_atr = 10;\n",
    "    for p in atr_periods_min: df[f'atr_{p}m'] = df['tr'].rolling(p, min_periods=min_p_atr).mean()\n",
    "    df = df.drop(columns=['prev_close', 'hml', 'hmpc', 'lmpc', 'tr', 'body_abs', 'range', f'vol_ma_{vol_ma_period}m', f'body_ma_{body_ma_period}m'], errors='ignore')\n",
    "    df['atr_norm'] = df[f'atr_{atr_periods_min[0]}m'] / (df['close'] + epsilon) if f'atr_{atr_periods_min[0]}m' in df else np.nan\n",
    "    # print(\"  Calculating TA features (RSI, Stoch, MACD, BBands)...\") # Optional debug\n",
    "    try:\n",
    "        min_ta_warmup = 30\n",
    "        if len(df) >= min_ta_warmup:\n",
    "            df.ta.rsi(length=14, append=True)\n",
    "            if 'RSI_14' in df.columns: df['rsi_14_oversold']=(df['RSI_14']<30).astype(int); df['rsi_14_overbought']=(df['RSI_14']>70).astype(int); df['rsi_ob_confirm']=((df['RSI_14']>70)&(df['close']>df['open'])).astype(int); df['rsi_os_confirm']=((df['RSI_14']<30)&(df['close']<df['open'])).astype(int)\n",
    "            df.ta.stoch(k=14, d=3, smooth_k=3, append=True)\n",
    "            if 'STOCHk_14_3_3' in df.columns: df['stoch_k_oversold']=(df['STOCHk_14_3_3']<20).astype(int); df['stoch_k_overbought']=(df['STOCHk_14_3_3']>80).astype(int)\n",
    "            df.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "            if 'MACDh_12_26_9' in df.columns: df['macd_hist_positive']=(df['MACDh_12_26_9']>0).astype(int); df['macd_hist_increasing']=(df['MACDh_12_26_9']>df['MACDh_12_26_9'].shift(1)).astype(int)\n",
    "            df.ta.bbands(length=20, std=2, append=True)\n",
    "            if 'BBP_20_2.0' in df.columns: df['bbp_near_upper']=(df['BBP_20_2.0']>0.9).astype(int); df['bbp_near_lower']=(df['BBP_20_2.0']<0.1).astype(int)\n",
    "            if 'RSI_14' in df.columns:\n",
    "                for n_div in [30]:\n",
    "                    if len(df) > n_div: min_price_n=df['low'].rolling(n_div,min_periods=n_div//2).min(); min_rsi_n=df['RSI_14'].rolling(n_div,min_periods=n_div//2).min(); price_lower_low=df['low']<min_price_n.shift(1); rsi_higher_low=df['RSI_14']>min_rsi_n.shift(1); df[f'rsi_bull_div_{n_div}m']=(price_lower_low&rsi_higher_low).astype(int)\n",
    "                    else: df[f'rsi_bull_div_{n_div}m']=0\n",
    "        # else: print(f\"  Warning: Insufficient data ({len(df)} rows) for TA warmup.\") # Optional debug\n",
    "    except Exception as e_ta: print(f\"!! Error calculating TA features: {e_ta}\")\n",
    "    cols_to_drop_intermediate = ['price_change_1m_temp', 'volume_return_1m']\n",
    "    df = df.drop(columns=[col for col in cols_to_drop_intermediate if col in df.columns], errors='ignore')\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    if df[numeric_cols].isin([np.inf, -np.inf]).any().any(): df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    # print(f\"  Feature Eng End: Total columns = {df.shape[1]}, Rows = {len(df)}\") # Optional debug\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Optuna Objective Function (Optimizing for PRECISION) ---\n",
    "# ==============================================================================\n",
    "def objective(trial, X, y, fixed_params, cv_strategy, k_neighbors):\n",
    "    \"\"\"Objective function for Optuna using SMOTE within CV folds, maximizing Precision@Threshold.\"\"\" # MODIFIED DOCSTRING\n",
    "\n",
    "    # Define hyperparameter search space (scale_pos_weight is removed)\n",
    "    param = {\n",
    "        \"max_depth\":        trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
    "        \"reg_alpha\":        trial.suggest_float(\"reg_alpha\", 1e-3, 0.5, log=True),\n",
    "        \"reg_lambda\":       trial.suggest_float(\"reg_lambda\", 1.0, 10.0, log=True),\n",
    "        \"gamma\":            trial.suggest_float(\"gamma\", 0, 0.5),\n",
    "        \"subsample\":        trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.03, 0.15, log=True),\n",
    "        # REMOVED 'scale_pos_weight'\n",
    "    }\n",
    "    xgb_params = {key: val for key, val in fixed_params.items() if key != 'scale_pos_weight'}\n",
    "    xgb_params.update(param)\n",
    "\n",
    "    cv_scores = [] # Store precision scores for this trial\n",
    "    try:\n",
    "        y_np = y.to_numpy() if isinstance(y, pd.Series) else np.array(y)\n",
    "        X_np = X.to_numpy() if isinstance(X, pd.DataFrame) else np.array(X)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X_np, y_np)):\n",
    "            X_train_fold, X_val_fold = X_np[train_idx], X_np[val_idx]\n",
    "            y_train_fold, y_val_fold = y_np[train_idx], y_np[val_idx]\n",
    "\n",
    "            # --- SMOTE ---\n",
    "            minority_class_count = np.sum(y_train_fold == 1)\n",
    "            X_train_resampled, y_train_resampled = X_train_fold, y_train_fold\n",
    "            if minority_class_count >= k_neighbors + 1:\n",
    "                try:\n",
    "                    smote = SMOTE(random_state=fixed_params.get(\"random_state\", 42) + fold, k_neighbors=k_neighbors)\n",
    "                    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "                except Exception as e_smote:\n",
    "                    print(f\"  SMOTE Error fold {fold+1}: {e_smote}\")\n",
    "            # --- End SMOTE ---\n",
    "\n",
    "            if len(np.unique(y_train_resampled)) < 2: cv_scores.append(0.0); continue\n",
    "\n",
    "            model = xgb.XGBClassifier(**xgb_params)\n",
    "            model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
    "\n",
    "            # --- MODIFIED EVALUATION: Use PRECISION ---\n",
    "            preds_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "            preds_binary = (preds_proba >= OPTUNA_EVAL_THRESHOLD).astype(int)\n",
    "            # Calculate Precision instead of F1\n",
    "            precision = precision_score(y_val_fold, preds_binary, zero_division=0)\n",
    "            cv_scores.append(precision)\n",
    "            # --- END MODIFICATION ---\n",
    "\n",
    "            # Optuna Pruning Integration\n",
    "            trial.report(precision, fold) # Report precision for pruning\n",
    "            if trial.should_prune(): raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Calculate the average PRECISION score across valid folds\n",
    "        average_score = np.mean(cv_scores) if cv_scores else 0.0 # Store average precision\n",
    "\n",
    "    except optuna.exceptions.TrialPruned: raise\n",
    "    except Exception as e: print(f\"Error in Optuna trial {trial.number}, fold {fold}: {e}\"); return 0.0\n",
    "    return average_score if not np.isnan(average_score) else 0.0\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Main Script Logic ---\n",
    "# ==============================================================================\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"--- Data Loading ---\"); print(f\"Loading data from: {CSV_FILE_PATH}...\")\n",
    "try:\n",
    "    df_data = pd.read_csv(CSV_FILE_PATH, parse_dates=['timestamp'])\n",
    "    df_data = df_data.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df_data)} rows for {SYMBOL_NAME}.\")\n",
    "except Exception as e: print(f\"Error loading data: {e}\"); exit()\n",
    "min_data_needed = TRAIN_WINDOW_MINUTES + TEST_WINDOW_MINUTES + STEP_MINUTES\n",
    "if len(df_data) < min_data_needed: print(f\"Error: Insufficient initial data ({len(df_data)} < {min_data_needed} needed).\"); exit()\n",
    "\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(f\"\\n--- Feature Engineering for {SYMBOL_NAME} ---\")\n",
    "start_fe = time.time()\n",
    "df_data = calculate_features_min_rare_event(df_data) # Use the function with new features\n",
    "if df_data.empty: print(f\"Error: Feature calculation resulted in empty DataFrame.\"); exit()\n",
    "print(f\"Feature engineering complete. Took {time.time() - start_fe:.2f}s.\")\n",
    "\n",
    "# --- 3. Define Target Variable ---\n",
    "print(\"\\n--- Target Definition ---\")\n",
    "print(f\"Defining target: {PREDICTION_WINDOW_MINUTES}m return >= {TARGET_THRESHOLD_PCT}%...\")\n",
    "target_col = f'target_return_{PREDICTION_WINDOW_MINUTES}m'\n",
    "df_data[target_col] = df_data['close'].shift(-PREDICTION_WINDOW_MINUTES).sub(df_data['close']).div(df_data['close'].replace(0, np.nan)).mul(100)\n",
    "target_occurrence = df_data[target_col].notna() & (df_data[target_col] >= TARGET_THRESHOLD_PCT)\n",
    "print(f\"  Raw positive target occurrence (before NaN drop): {target_occurrence.mean()*100:.2f}%\")\n",
    "\n",
    "# --- 4. Prepare Data for Modeling ---\n",
    "print(\"\\n--- Data Preparation ---\")\n",
    "base_cols_ohlcv = ['open', 'high', 'low', 'close', 'volumefrom', 'volumeto']\n",
    "cols_to_keep_final = ['timestamp', target_col]\n",
    "potential_feature_cols = [col for col in df_data.columns if col not in cols_to_keep_final and col not in base_cols_ohlcv and not col.startswith('STOCHd') and not col.startswith('MACDs')]\n",
    "numeric_feature_cols = df_data[potential_feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "final_feature_cols = numeric_feature_cols\n",
    "if not final_feature_cols: print(\"Error: No numeric features found after selection.\"); exit()\n",
    "cols_to_select = final_feature_cols + [col for col in cols_to_keep_final if col in df_data.columns]\n",
    "df_model_ready = df_data[cols_to_select].copy()\n",
    "\n",
    "print(\"Applying final NaN/Inf Handling...\")\n",
    "initial_rows = len(df_model_ready)\n",
    "df_model_ready = df_model_ready.dropna(subset=final_feature_cols + [target_col])\n",
    "final_rows = len(df_model_ready)\n",
    "print(f\"NaN Handling: Dropped {initial_rows - final_rows} rows with NaNs.\")\n",
    "if df_model_ready[final_feature_cols].isin([np.inf, -np.inf]).any().any():\n",
    "    inf_count = df_model_ready[final_feature_cols].isin([np.inf, -np.inf]).sum().sum()\n",
    "    print(f\"Replacing {inf_count} final infinites...\")\n",
    "    df_model_ready.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    rows_b4 = len(df_model_ready); df_model_ready = df_model_ready.dropna(subset=final_feature_cols)\n",
    "    print(f\"Dropped {rows_b4 - len(df_model_ready)} more rows after Inf handling.\")\n",
    "if df_model_ready.empty: print(f\"Error: DataFrame empty after final NaN/Inf handling.\"); exit()\n",
    "\n",
    "X = df_model_ready[final_feature_cols]\n",
    "y_binary = (df_model_ready[target_col] >= TARGET_THRESHOLD_PCT).astype(int)\n",
    "timestamps = df_model_ready['timestamp']\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}, Target shape: {y_binary.shape}\")\n",
    "print(f\"Using {len(final_feature_cols)} features.\")\n",
    "print(f\"Positive Target Rate in Final Data: {y_binary.mean()*100:.2f}%\") # Check final balance\n",
    "\n",
    "# --- 5. SLIDING Window Backtesting ---\n",
    "print(f\"\\n--- Starting SLIDING Window Backtest for {SYMBOL_NAME} ---\")\n",
    "print(f\"!!! Using Optuna (TimeSeriesSplit CV + SMOTE, optimizing Precision) + Rare Event Features !!!\") # <-- Updated print\n",
    "\n",
    "if len(X) < TRAIN_WINDOW_MINUTES + STEP_MINUTES:\n",
    "     print(f\"Error: Not enough data after pre-processing ({len(X)} rows) for train window ({TRAIN_WINDOW_MINUTES}) + step ({STEP_MINUTES}).\"); exit()\n",
    "\n",
    "all_predictions_proba = []; all_actual = []; backtest_timestamps = []\n",
    "all_best_params = []\n",
    "num_steps = 0\n",
    "start_index_loop = TRAIN_WINDOW_MINUTES\n",
    "end_index_loop = len(X) - TEST_WINDOW_MINUTES + 1\n",
    "\n",
    "print(f\"Train Window: {TRAIN_WINDOW_MINUTES}m, Step: {STEP_MINUTES}m, Test Window: {TEST_WINDOW_MINUTES}m, Optuna Trials: {N_OPTUNA_TRIALS}, CV Splits: {OPTUNA_CV_SPLITS}\")\n",
    "loop_start_time = time.time()\n",
    "\n",
    "for i in range(start_index_loop, end_index_loop, STEP_MINUTES):\n",
    "    step_start_time = time.time()\n",
    "    train_idx_start = i - TRAIN_WINDOW_MINUTES\n",
    "    train_idx_end = i\n",
    "    test_idx_start = i\n",
    "    test_idx_end = min(i + TEST_WINDOW_MINUTES, len(X))\n",
    "\n",
    "    if test_idx_start >= test_idx_end: print(f\"Stopping loop: Test window invalid.\"); break\n",
    "\n",
    "    X_train_roll = X.iloc[train_idx_start : train_idx_end]\n",
    "    y_train_roll = y_binary.iloc[train_idx_start : train_idx_end]\n",
    "    X_test_roll = X.iloc[test_idx_start : test_idx_end]\n",
    "    y_test_roll_actual_series = y_binary.iloc[test_idx_start : test_idx_end]\n",
    "    step_timestamps = timestamps.iloc[test_idx_start : test_idx_end]\n",
    "\n",
    "    if y_test_roll_actual_series.empty: print(f\"Warning: Step {i}, empty test actuals.\"); continue\n",
    "    current_timestamp = step_timestamps.iloc[0]\n",
    "    if X_train_roll.empty or len(np.unique(y_train_roll)) < 2: print(f\"Warning: Step {i}, invalid training data.\"); continue\n",
    "\n",
    "    print(f\"\\n--- Step {num_steps + 1} ({current_timestamp}) ---\")\n",
    "    print(f\"  Train: [{train_idx_start}:{train_idx_end-1}]; Test: [{test_idx_start}:{test_idx_end-1}]\")\n",
    "    print(f\"  Training data balance before SMOTE: {y_train_roll.mean()*100:.2f}% positive\")\n",
    "\n",
    "    # --- Hyperparameter Tuning with Optuna ---\n",
    "    print(f\"  Running Optuna ({N_OPTUNA_TRIALS} trials, cv={OPTUNA_CV_SPLITS} TimeSeriesSplit+SMOTE, scoring Precision@{OPTUNA_EVAL_THRESHOLD})...\") # <-- Updated print\n",
    "    optuna_start_time = time.time()\n",
    "    try:\n",
    "        cv_strategy = TimeSeriesSplit(n_splits=OPTUNA_CV_SPLITS)\n",
    "        pruner = optuna.pruners.MedianPruner(n_warmup_steps=5, n_min_trials=10)\n",
    "        sampler = optuna.samplers.TPESampler(seed=i)\n",
    "        study = optuna.create_study(direction='maximize', pruner=pruner, sampler=sampler) # Maximize Precision\n",
    "        obj_func = lambda trial: objective(trial, X_train_roll, y_train_roll, XGB_FIXED_PARAMS, cv_strategy, SMOTE_K_NEIGHBORS)\n",
    "\n",
    "        study.optimize(obj_func, n_trials=N_OPTUNA_TRIALS, n_jobs=1, show_progress_bar=False)\n",
    "\n",
    "        best_params_step = study.best_params\n",
    "        best_score_step = study.best_value # Best average Precision from CV\n",
    "\n",
    "        print(f\"  Optuna finished in {time.time() - optuna_start_time:.2f}s.\")\n",
    "        print(f\"  Best Params: {best_params_step}, Best CV Precision(@{OPTUNA_EVAL_THRESHOLD}): {best_score_step:.4f}\") # <-- Updated print\n",
    "        all_best_params.append({'step': num_steps + 1, 'params': best_params_step, 'cv_precision': best_score_step, 'timestamp': current_timestamp}) # <-- Updated key\n",
    "\n",
    "        # --- Fit final model for the step ---\n",
    "        final_model_params = {**XGB_FIXED_PARAMS, **best_params_step}\n",
    "        print(\"  Fitting final model on original training data for step...\")\n",
    "        model_roll = xgb.XGBClassifier(**final_model_params)\n",
    "        model_roll.fit(X_train_roll, y_train_roll, verbose=False)\n",
    "\n",
    "        # --- Predict probabilities for the test window ---\n",
    "        prob_roll_window = model_roll.predict_proba(X_test_roll)[:, 1]\n",
    "\n",
    "        # --- Store results ---\n",
    "        all_predictions_proba.extend(prob_roll_window)\n",
    "        all_actual.extend(y_test_roll_actual_series.tolist())\n",
    "        backtest_timestamps.extend(step_timestamps.tolist())\n",
    "        num_steps += 1\n",
    "\n",
    "    except ValueError as ve: print(f\"!! Value Error at step {i}: {ve}\"); continue\n",
    "    except Exception as e_step: print(f\"!! Error at step {i}: {e_step}\"); traceback.print_exc(); continue\n",
    "\n",
    "    step_end_time = time.time()\n",
    "    print(f\"  Step {num_steps} finished in {step_end_time - step_start_time:.2f}s total.\")\n",
    "\n",
    "loop_end_time = time.time()\n",
    "print(f\"\\nBacktesting loop finished. Completed {num_steps} steps in {(loop_end_time - loop_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# --- 6. Evaluate Backtesting Results with PTT ---\n",
    "# NOTE: Final PTT evaluation still optimizes for F1 score by default.\n",
    "# You could change the logic here to optimize for Precision or another metric if desired.\n",
    "if num_steps > 0 and len(all_predictions_proba) == len(all_actual) and len(all_predictions_proba) == len(backtest_timestamps):\n",
    "    print(f\"\\n--- Evaluating Results for {SYMBOL_NAME} with Probability Threshold Tuning (Optimizing F1 overall) ---\") # Clarified PTT goal\n",
    "    print(f\"Threshold search range: {THRESHOLD_SEARCH_RANGE}\")\n",
    "    best_threshold_f1 = 0.5; best_f1_overall = -1.0 # Vars specific to F1 PTT\n",
    "    results_per_threshold = {}\n",
    "    probabilities_np = np.array(all_predictions_proba)\n",
    "    actual_np = np.array(all_actual)\n",
    "\n",
    "    for t in THRESHOLD_SEARCH_RANGE:\n",
    "        predictions_thresh = (probabilities_np >= t).astype(int)\n",
    "        # Calculate all metrics for reporting\n",
    "        if np.sum(actual_np) == 0 and np.sum(predictions_thresh) == 0: acc_t, pre_t, rec_t, f1_t = 1.0, 1.0, 1.0, 1.0\n",
    "        elif np.sum(actual_np) > 0 and np.sum(predictions_thresh) == 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        elif np.sum(actual_np) == 0 and np.sum(predictions_thresh) > 0: acc_t = accuracy_score(actual_np, predictions_thresh); pre_t, rec_t, f1_t = 0.0, 0.0, 0.0\n",
    "        else:\n",
    "             acc_t = accuracy_score(actual_np, predictions_thresh); pre_t = precision_score(actual_np, predictions_thresh, zero_division=0)\n",
    "             rec_t = recall_score(actual_np, predictions_thresh, zero_division=0); f1_t = f1_score(actual_np, predictions_thresh, zero_division=0)\n",
    "        results_per_threshold[round(t, 2)] = {'f1': f1_t, 'acc': acc_t, 'pre': pre_t, 'rec': rec_t}\n",
    "\n",
    "        # Find best threshold based on F1 score\n",
    "        if f1_t >= best_f1_overall:\n",
    "             if f1_t > best_f1_overall or abs(t - 0.5) < abs(best_threshold_f1 - 0.5): # Tie-break for F1\n",
    "                  best_f1_overall = f1_t; best_threshold_f1 = t\n",
    "\n",
    "    print(f\"\\nBest Threshold for {SYMBOL_NAME} (Maximizing F1): {best_threshold_f1:.2f} (Yielding F1 Score: {best_f1_overall:.4f})\")\n",
    "\n",
    "    # Calculate final metrics using the threshold that optimized F1\n",
    "    final_predictions_optimized = (probabilities_np >= best_threshold_f1).astype(int)\n",
    "    final_accuracy = accuracy_score(actual_np, final_predictions_optimized)\n",
    "    final_precision = precision_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_recall = recall_score(actual_np, final_predictions_optimized, zero_division=0)\n",
    "    final_f1 = f1_score(actual_np, final_predictions_optimized, zero_division=0) # This is best_f1_overall\n",
    "\n",
    "    print(f\"\\n--- Final Performance Metrics for {SYMBOL_NAME} (Threshold Optimized for F1) ---\")\n",
    "    print(f\"Target: {PREDICTION_WINDOW_MINUTES}m return >= {TARGET_THRESHOLD_PCT}%\")\n",
    "    print(f\"Windowing: Train={TRAIN_WINDOW_MINUTES}m, Step={STEP_MINUTES}m, Test Window={TEST_WINDOW_MINUTES}m\")\n",
    "    print(f\"Hyperparameter Tuning: Optuna ({N_OPTUNA_TRIALS} trials/step, TSS CV+SMOTE, optimizing Precision@{OPTUNA_EVAL_THRESHOLD})\") # Updated print\n",
    "    print(f\"Total Individual Predictions Evaluated: {len(actual_np)}\")\n",
    "    print(f\"Positive Target Occurrence (final eval set): {actual_np.mean()*100:.2f}%\")\n",
    "    print(f\"Overall Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"Overall Precision: {final_precision:.4f}\")\n",
    "    print(f\"Overall Recall:    {final_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score:  {final_f1:.4f}\")\n",
    "\n",
    "    # Report metrics specifically at the threshold Optuna was evaluating\n",
    "    if OPTUNA_EVAL_THRESHOLD in results_per_threshold:\n",
    "        res_eval = results_per_threshold[OPTUNA_EVAL_THRESHOLD]\n",
    "        print(f\"(Metrics @ Optuna Eval Thresh {OPTUNA_EVAL_THRESHOLD}: F1:{res_eval['f1']:.4f}, Acc:{res_eval['acc']:.4f}, Pre:{res_eval['pre']:.4f}, Rec:{res_eval['rec']:.4f})\") # <-- ADDED/MODIFIED\n",
    "    elif 0.5 in results_per_threshold: # Fallback to show 0.5 if eval thresh wasn't hit\n",
    "        res_def = results_per_threshold[0.5]\n",
    "        print(f\"(Compare: Default 0.5 Thresh -> F1:{res_def['f1']:.4f}, Acc:{res_def['acc']:.4f}, Pre:{res_def['pre']:.4f}, Rec:{res_def['rec']:.4f})\")\n",
    "\n",
    "    results_summary = { # Store results\n",
    "        'symbol': SYMBOL_NAME, 'probabilities': probabilities_np, 'actuals': actual_np, 'timestamps': backtest_timestamps,\n",
    "        'best_threshold_f1': best_threshold_f1, # Store the best F1 threshold\n",
    "        'metrics_optimized_f1': {'acc': final_accuracy, 'pre': final_precision, 'rec': final_recall, 'f1': final_f1},\n",
    "        'metrics_at_optuna_eval_thresh': results_per_threshold.get(OPTUNA_EVAL_THRESHOLD, {}), # Store metrics at Optuna's eval threshold\n",
    "        'best_params_per_step': all_best_params, 'results_per_threshold': results_per_threshold\n",
    "    }\n",
    "\n",
    "    # --- 7. Plot Cumulative Accuracy ---\n",
    "    print(f\"\\nPlotting cumulative accuracy for {SYMBOL_NAME} (using threshold optimized for F1)...\")\n",
    "    try:\n",
    "        # Plotting uses final_predictions_optimized (based on best_threshold_f1)\n",
    "        cumulative_accuracy_list_optimized = (np.cumsum(final_predictions_optimized == actual_np) / np.arange(1, len(actual_np) + 1))\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(backtest_timestamps, cumulative_accuracy_list_optimized, marker='.', linestyle='-', markersize=1, alpha=0.7, label=f'Cumulative Accuracy ({SYMBOL_NAME}) @ F1 Threshold') # Clarify label\n",
    "        rolling_window_plot_size = max(TEST_WINDOW_MINUTES * 5, 60 * 12)\n",
    "        if len(actual_np) > rolling_window_plot_size:\n",
    "             results_df = pd.DataFrame({'correct': (final_predictions_optimized == actual_np).astype(int)}, index=pd.to_datetime(backtest_timestamps))\n",
    "             try: rolling_acc = results_df['correct'].rolling(window=rolling_window_plot_size, min_periods=rolling_window_plot_size//2).mean()\n",
    "             except Exception as e_roll: print(f\"Could not calculate rolling accuracy: {e_roll}\"); rolling_acc = None\n",
    "             if rolling_acc is not None: plt.plot(rolling_acc.index, rolling_acc, linestyle='--', color='red', label=f'Rolling Acc ({rolling_window_plot_size} min window)')\n",
    "        plt.title(f'{SYMBOL_NAME} Backtest (Optuna Prec Opt, TSS CV+SMOTE) - Best F1 Thresh: {best_threshold_f1:.2f}') # Update title\n",
    "        plt.xlabel('Timestamp'); plt.ylabel('Accuracy'); min_y_plot=max(0.0, np.min(cumulative_accuracy_list_optimized)-0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.4)\n",
    "        max_y_plot=min(1.0, np.max(cumulative_accuracy_list_optimized)+0.05 if len(cumulative_accuracy_list_optimized)>0 else 0.8)\n",
    "        if max_y_plot - min_y_plot < 0.1: mid_point=(max_y_plot+min_y_plot)/2; min_y_plot=max(0.0, mid_point-0.05); max_y_plot=min(1.0, mid_point+0.05)\n",
    "        plt.ylim(min_y_plot, max_y_plot); plt.grid(True, linestyle='--', alpha=0.6); plt.legend(); plt.xticks(rotation=30, ha='right'); plt.tight_layout()\n",
    "        plot_filename = f\"backtest_accuracy_{SYMBOL_NAME}_30m_target_Optuna_Prec_SMOTE.png\" # Update filename\n",
    "        plt.savefig(plot_filename); print(f\"Saved accuracy plot to {plot_filename}\"); plt.close()\n",
    "    except Exception as e_plot: print(f\"Error plotting: {e_plot}\")\n",
    "\n",
    "elif num_steps == 0: print(f\"No backtesting steps completed.\")\n",
    "else: print(f\"Error: Length mismatch in results arrays. Cannot evaluate.\")\n",
    "\n",
    "# --- End of Script ---\n",
    "print(f\"\\n{'='*30} Overall Script Finished for {SYMBOL_NAME} {'='*30}\")\n",
    "overall_end_time = time.time()\n",
    "print(f\"Total execution time: {(overall_end_time - overall_start_time)/60:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
